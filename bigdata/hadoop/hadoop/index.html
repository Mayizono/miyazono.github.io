<!-- build time:Thu May 22 2025 10:03:32 GMT+0800 (中国标准时间) --><!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="alternate" type="application/rss+xml" title="冬樱茶" href="https://github.com/Mayizono/miyazono.github.io/rss.xml"><link rel="alternate" type="application/atom+xml" title="冬樱茶" href="https://github.com/Mayizono/miyazono.github.io/atom.xml"><link rel="alternate" type="application/json" title="冬樱茶" href="https://github.com/Mayizono/miyazono.github.io/feed.json"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/css/app.css?v=0.2.5"><link rel="canonical" href="https://github.com/Mayizono/miyazono.github.io/bigdata/hadoop/hadoop/"><title>Hadoop 学习 - Hadoop - 大数据 | Yume Shoka = 冬樱茶</title><meta name="generator" content="Hexo 5.4.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">Hadoop 学习</h1><div class="meta"><span class="item" title="创建时间：2019-09-11 00:00:00"><span class="icon"><i class="ic i-calendar"></i> </span><span class="text">发表于</span> <time itemprop="dateCreated datePublished" datetime="2019-09-11T00:00:00+08:00">2019-09-11</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span class="text">本文字数</span> <span>5.6k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span class="text">阅读时长</span> <span>5 分钟</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">Yume Shoka</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div id="imgs" class="pjax"><ul><li class="item" data-background-image="https://i3.wp.com/wx4.sinaimg.cn/large/6833939bly1gicljgocqbj20zk0m8e81.jpg"></li><li class="item" data-background-image="https://i3.wp.com/wx4.sinaimg.cn/large/6833939bly1gicm0fdw5cj20zk0m8hdt.jpg"></li><li class="item" data-background-image="https://i3.wp.com/wx4.sinaimg.cn/large/6833939bly1giclfdu6exj20zk0m87hw.jpg"></li><li class="item" data-background-image="https://i3.wp.com/wx4.sinaimg.cn/large/6833939bly1gicli9lfebj20zk0m84qp.jpg"></li><li class="item" data-background-image="https://i3.wp.com/wx4.sinaimg.cn/large/6833939bly1giclgrvbd6j20zk0m8qv5.jpg"></li><li class="item" data-background-image="https://i3.wp.com/wx4.sinaimg.cn/large/6833939bly1giclj61ylzj20zk0m8b29.jpg"></li></ul></div><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div></header><main><div class="inner"><div id="main" class="pjax"><div class="article wrap"><div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i> <span><a href="/">首页</a></span><i class="ic i-angle-right"></i> <span itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/bigdata/" itemprop="item" rel="index" title="分类于 大数据"><span itemprop="name">大数据</span></a><meta itemprop="position" content="1"></span><i class="ic i-angle-right"></i> <span class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/bigdata/hadoop/" itemprop="item" rel="index" title="分类于 Hadoop"><span itemprop="name">Hadoop</span></a><meta itemprop="position" content="2"></span></div><article itemscope itemtype="http://schema.org/Article" class="post block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://github.com/Mayizono/miyazono.github.io/bigdata/hadoop/hadoop/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="Miyazono"><meta itemprop="description" content=", "></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="冬樱茶"></span><div class="body md" itemprop="articleBody"><h1 id="hadoop"><a class="anchor" href="#hadoop">#</a> hadoop</h1><h2 id="入门"><a class="anchor" href="#入门">#</a> 入门</h2><h3 id="概论"><a class="anchor" href="#概论">#</a> 概论</h3><ul><li><p>概念</p><ul><li>处理海量数据的存储和分析计算问题</li></ul></li><li><p>特点</p><ul><li>大量、高速、多样、低密度价值</li></ul></li><li><p>组织架构</p><ul><li>数据仓库组（ETL 和数据分析）</li><li>实时组（实时指标分析、性能调优）</li></ul></li></ul><h3 id="hadoop框架"><a class="anchor" href="#hadoop框架">#</a> hadoop 框架</h3><ul><li><p>hadoop 是什么</p><ul><li><p>Apache 开发的分布式系统基础架构</p></li><li><p>一个生态圈</p><ul><li>mahout</li><li>zookeeper</li><li>pig</li><li>hbase</li><li>....</li></ul></li><li><p>解决海量数据的存储和分析计算</p></li></ul></li><li><p>三大发行版</p><ul><li>Apache、Cloudera、Hortonworks</li></ul></li><li><p>优势</p><ul><li>高可靠：维护了多个副本</li><li>高拓展性：可根据需求进行增减服务器</li><li>高效性：task 并行执行</li><li>高容错性：失败的任务会进行重新分配</li></ul></li><li><p>组成</p><ul><li><p>hadoop1.x</p><ul><li>common ：辅助工具</li><li>hdfs ：数据的存储</li><li>MapReduce ：计算 + 资源调度</li></ul></li><li><p>hadoop2.x</p><ul><li>common ：辅助工具</li><li>hdfs ： 数据的存储</li><li>map ：计算</li><li>reduce ： 资源调度</li></ul></li></ul></li><li><p>架构概述</p><ul><li><p>HDFS</p><ul><li>namenode : 存储文件的元数据</li><li>datanode ：存储文件的真实数据 + 块数据的校验和</li><li>secondarynamenode ：对 namenode 的元数据进行备份</li></ul></li><li><p>YARN</p><ul><li><p>resourcemanager</p><ul><li>处理客户端的请求</li><li>监控 nodemanager</li><li>启动 applicationmaster</li><li>资源分配与调度</li></ul></li><li><p>nodemanager</p><ul><li>管理单个节点上的资源</li><li>处理来自 resourcemanager 的命令</li><li>处理 applicationmaster 的命令</li></ul></li><li><p>container</p><ul><li>封装单个节点上多维资源：如 cpu、网络等</li></ul></li><li><p>applicationmaster</p><ul><li>负责数据的切分</li><li>为应用程序申请资源并分配给内部的任务</li><li>任务的监控和容错</li></ul></li></ul></li><li><p>MapReduce</p><ul><li>map 阶段并行处理输入的数据</li><li>对 map 结果进行汇总</li></ul></li></ul></li></ul><h3 id="hadoop运行模式"><a class="anchor" href="#hadoop运行模式">#</a> hadoop 运行模式</h3><ul><li><p>本地运行</p></li><li><p>伪分布式模式</p></li><li><p>完全分布式模式</p><ul><li><p>常用端口</p><ul><li><p>namenode</p><ul><li>web: 9870</li><li>内部通信：8020</li></ul></li><li><p>2NN</p><ul><li>web: 9868</li></ul></li><li><p>yarn</p><ul><li>web: 8088</li></ul></li><li><p>historyserver</p><ul><li>19888</li></ul></li><li><p>datanode</p><ul><li>web: 9864</li><li>内部通信：9866</li></ul></li></ul></li><li><p>出现错误看日志，看日志，看日志</p></li><li><p>格式化问题</p><ul><li>当出现宕机或删除以往数据时才需要进行格式</li><li>需删除所有节点的 data 和 log 数据</li></ul></li><li><p>启动集群</p><ul><li>start/stop-yarn.sh</li><li>yarn --daemon start/stop rm/nm</li><li>start/stop-dfs.sh</li><li>dfs --daemon start/stop nn/2nn/dn</li></ul></li><li><pre><code>                            执行wordcount程序
</code></pre></li></ul><p>hadoop jar jar 位置 wordcount 输入路径 输出路径（一定不能存在）</p><ul><li><p>配置文件说明</p><ul><li>core-default.xm</li><li>hdfs-default.xml</li><li>yarn-default.xml</li><li>mapred-default.xml</li></ul></li><li><p>拷贝文件</p><ul><li><p>scp</p><p>1）基本语法</p><p>scp -r <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mi>d</mi><mi>i</mi><mi>r</mi><mi mathvariant="normal">/</mi></mrow><annotation encoding="application/x-tex">pdir/</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal">p</span><span class="mord mathnormal">d</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:.02778em">r</span><span class="mord">/</span></span></span></span>fname <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mi>s</mi><mi>e</mi><mi>r</mi><mi mathvariant="normal">@</mi><mi>h</mi><mi>a</mi><mi>d</mi><mi>o</mi><mi>o</mi><mi>p</mi></mrow><annotation encoding="application/x-tex">user@hadoop</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord mathnormal">u</span><span class="mord mathnormal">s</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:.02778em">r</span><span class="mord">@</span><span class="mord mathnormal">h</span><span class="mord mathnormal">a</span><span class="mord mathnormal">d</span><span class="mord mathnormal">o</span><span class="mord mathnormal">o</span><span class="mord mathnormal">p</span></span></span></span>host:<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mi>d</mi><mi>i</mi><mi>r</mi><mi mathvariant="normal">/</mi></mrow><annotation encoding="application/x-tex">pdir/</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal">p</span><span class="mord mathnormal">d</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:.02778em">r</span><span class="mord">/</span></span></span></span>fname</p><p>命令 递归 要拷贝的文件路径 / 名称 目的用户 @主机：目的路径 / 名称</p><ol start="2"><li><p>案例实操</p><p>前提：在 Hadoop102 hadoop103 hadoop104 都已经创建好的 /opt/module</p><pre><code>  /opt/software 两个目录， 并且已经把这两个目录修改为miyazono:miyazono
</code></pre></li></ol><ul><li>全部复制</li></ul></li><li><p>rsync</p><p>rsync -av <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mi>d</mi><mi>i</mi><mi>r</mi><mi mathvariant="normal">/</mi></mrow><annotation encoding="application/x-tex">pdir/</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal">p</span><span class="mord mathnormal">d</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:.02778em">r</span><span class="mord">/</span></span></span></span>fname <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mi>s</mi><mi>e</mi><mi>r</mi><mi mathvariant="normal">@</mi><mi>h</mi><mi>a</mi><mi>d</mi><mi>o</mi><mi>o</mi><mi>p</mi></mrow><annotation encoding="application/x-tex">user@hadoop</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord mathnormal">u</span><span class="mord mathnormal">s</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:.02778em">r</span><span class="mord">@</span><span class="mord mathnormal">h</span><span class="mord mathnormal">a</span><span class="mord mathnormal">d</span><span class="mord mathnormal">o</span><span class="mord mathnormal">o</span><span class="mord mathnormal">p</span></span></span></span>host:<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mi>d</mi><mi>i</mi><mi>r</mi><mi mathvariant="normal">/</mi></mrow><annotation encoding="application/x-tex">pdir/</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal">p</span><span class="mord mathnormal">d</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:.02778em">r</span><span class="mord">/</span></span></span></span>fname</p><p>命令 选项参数 要拷贝的文件路径 / 名称 目的用户 @主机：目的路径 / 名称</p><ul><li>只复制差异部分</li></ul></li><li><p>使用脚本进行分发</p><ul><li>创建在 path 环境变量下</li><li>需要执行权限</li></ul></li></ul></li><li><p>无密码登录</p><ul><li>ssh 另外一台电脑的 ip 地址</li><li>ssh-keygen -t rea 生成公钥和私钥</li><li>ssh-copy-id 另外一台电脑 ip 地址：将公钥提供给另外一台机器</li><li>ssh 本机 ip 也需要密码，所以也需要进行公钥的分发</li><li>无密码登入是关联到具体的用户，如 miyazono 进行公钥和私钥的分发，root 连接其他电脑时，依然需要输入密码</li><li>无密码登录是单向的，如需要相互连接，则需要相互分发自己的公钥</li></ul></li><li><p>配置文件 workers (datanode 的节点) 中，该文件中的内部结尾部分不能有空格，文中不能有空行</p></li></ul></li></ul><h3 id="hadoop3x与2x的区别"><a class="anchor" href="#hadoop3x与2x的区别">#</a> hadoop3.x 与 2.x 的区别</h3><ul><li>java 从 7 升级到了 8</li><li>引入了纠删码</li><li>重写了 shell 脚本</li><li>支持超过两个 NN</li><li>许多服务的默认端口改变了</li><li>处理海量数据的存储和分析计算问题</li></ul><h2 id="hdfs"><a class="anchor" href="#hdfs">#</a> HDFS</h2><h3 id="概论-2"><a class="anchor" href="#概论-2">#</a> 概论</h3><ul><li><p>hdfs 概念</p><ul><li>是一种分布式文件管理系统</li><li>通过目录树定位文件</li><li>适合一次写入，多次读出的场景</li><li>仅支持数据的追加，不支持文件的随机修改</li><li>不适合处理小文件</li><li>适合适当延时的处理</li></ul></li><li><p>框架</p><ul><li><p>namenode</p><ul><li>管理 hdfs 的名称空间</li><li>配置副本策略</li><li>管理数据块的映射信息</li><li>处理客户端的读写请求</li></ul></li><li><p>datanode</p><ul><li>存储实际的数据块</li><li>执行数据块的读写操作</li></ul></li><li><p>client</p><ul><li>文件的切分，文件上传时，client 根据块的大小将文件进行切分</li><li>与 namenode 交互，获取文件的位置信息</li><li>与 datanode 交互，读取或写入数据</li><li>client 提供了一些命令来管理 hdfs，比如格式化 namenode</li><li>client 可以通过一些命令访问 hdfs，如对 hdfs 进行增删改查</li></ul></li><li><p>secondarynamenode</p><ul><li>不是 nn 的热备，当 nn 挂掉时，它不能替代 nn 并提供服务</li><li>辅助 nn，分担其工作量，比如定期合并镜像文件和编辑日志，并推送给 nn</li><li>可恢复 nn 的数据（可能会丢失少部分）</li></ul></li></ul></li><li><p>文件块大小</p><ul><li>与文件的传输速度相关</li><li>文件大小 = 文件查询时间 * 100 * 文件传输速度<br>（取整，128，256）</li><li>文件块过小，增加了寻址时间，文件块过大，增加了数据传输时间</li></ul></li></ul><h3 id="hdfs之shell操作"><a class="anchor" href="#hdfs之shell操作">#</a> HDFS 之 shell 操作</h3><ul><li><p>格式</p><ul><li>hdfs dfs + 命令</li><li>hadoop fs + 命令</li></ul></li><li><p>具体命令</p><ul><li>-help 输出这个命令的参数</li><li>-ls ：显示目录信息</li><li><ul><li>mkdir 在 hdfs 上创建文件夹</li></ul></li><li>-moveFromLocal ：从本地剪切到 hdfs 上</li><li><pre><code>                  -put ： 从本地复制到hdfs上
</code></pre></li></ul></li><li><p>copyFromLocal ：从本地复制到 hdfs 上</p><ul><li><pre><code>            -get：从hdfs拷贝到本地
</code></pre></li></ul><p>-copyTolocal ：从 hdfs 拷贝到本地</p><ul><li>-appendToFile ：追加一个本地文件到 hdfs 已经存在的文件末尾</li><li>-cat ：显示文件内容</li><li><ul><li>chgrp 、 -chmod 、 -chown ：修改文件的权限</li></ul></li><li>-cp ：从 hdfs 的一个路径拷贝到 hdfs 另外一个路径</li><li>-mv ：从 hdfs 目录中移动文件</li><li>-tail ：显示一个文件的末尾</li><li>-rm -r：递归删除文件或文件夹</li><li><ul><li>rmdir ：删除空目录</li></ul></li><li>-du ：统计文件夹的大小信息</li><li>-setrep : 设置 hdfs 中文件的副本数量</li></ul></li></ul><h3 id="hdfs客户端操作"><a class="anchor" href="#hdfs客户端操作">#</a> hdfs 客户端操作</h3><ul><li><p>获取文件系统的对象</p><ul><li>获取 uri，uri = new URI (&quot;hdfs://hadoop102:9820&quot;)</li><li>获取配置文件对象 conf ，conf=new Configuration ();</li><li>获取登入的用户 user ，user= &quot;miyazono&quot;</li><li>获取文件系统对象，FileSystem.get (uri, conf, user)</li></ul></li><li><p>具体操作</p><ul><li>所有的输入和输出路径： new path (&quot;路径&quot;)</li><li>下载：copyFromLocalFile ()</li><li>上传 ： copyToLocalFile ()</li><li>liststatus (): 获取指定路径下文件或文件夹</li><li>listFiles () : 递归获取指定路径下所有的文件</li><li>delete () : 删除文件或文件夹</li></ul></li><li><p>关闭连接</p><ul><li>fs.close();</li></ul></li></ul><h3 id="hdfs的数据流"><a class="anchor" href="#hdfs的数据流">#</a> HDFS 的数据流</h3><ul><li><p>文件的写入</p><ul><li><p>客户端通过 hdfs 的对象向 namenode 请求上传文件</p></li><li><p>namenode 确定目标文件和父目录是否存在，返回可以上传文件</p></li><li><p>客户端请求上传第一个块，请求 namenode 返回 datanode</p></li><li><p>namenode 返回 3 个 datanode 节点，分别是 dn1，dn2，dn3</p><ul><li><p>关于副本的选择<br>（即 dn1、dn2、dn3）</p><ul><li>第一个副本：在 client 所处的节点上，如果客户端在集群外，随机选一个</li><li>第二个副本：在另一个机架的随机一个节点</li><li>第三个副本：在第二个副本所在的机架的随机节点</li></ul></li></ul></li><li><p>客户端通过 hdfs 的输出流请求某一个 dn 上传数据<br>(假设是 dn1，实则是根据网路拓扑节点距离计算得来)</p><ul><li>节点距离：两个节点到大最近的共同祖先的距离总和</li></ul></li><li><p>此 dn 会调用 dn2，dn2 调用 dn3，将这个通信通道建立完成</p></li><li><p>三个节点逐级应答客户端</p></li><li><p>客户端向 dn1 上传第一个块，以 packet 为单位，dn1 收到 packet 后，首先进行落盘处理，同时将收到的 packet 传递给到 dn2，dn2 做和 dn1 同样的动作将 packet 传递给到 dn3，dn3 直接落盘处理。</p><ul><li>传输的过程中，dn1 会将 packet 放进一个应答队列，待所有的节点均相应后，则表示此 packet 上传完成，否则默认未完成上传</li></ul></li><li><p>当一个块传输完成之后，客户端再次请求 namenode 上传第二个块，此时重复 4-8 步骤</p></li><li><p>待所有的数据块传输完成以后，则客户端关闭输出流。</p></li></ul></li><li><p>文件的写出</p><ul><li><p>客户端获取 hdfs 的对象，向 namenode 请求下载某某文件</p></li><li><p>namenode 返回该文件的元数据信息，所有块所在的节点</p></li><li><p>客户端获取文件系统的输入流，随机挑选一个 dn，请求读取数据</p><ul><li>就近 + 随机原则</li></ul></li><li><p>databnode 以 packet 为单位开始传输数据给客户端</p></li><li><p>客户端以 packet 为单位接收，先在本地缓存，然后写入目标文件</p></li><li><p>客户端根据元数据信息，请求 dn 读取下一个块信息，重复 3-6 步骤，直到所有的数据库读取完成</p></li></ul></li></ul><h3 id="nn和2nn"><a class="anchor" href="#nn和2nn">#</a> NN 和 2NN</h3><ul><li><p>机制</p><ul><li>第一次启动 namenode 后，首先会将磁盘中的镜像文件（fsimage）和编辑日志（edits）加载到内存中，如果是格式后的第一次启动，则是创建镜像文件和编辑日志</li><li>客户端对 hdfs 进行增删改操作</li><li>namenode 先将操作记录到编辑日志中</li><li>namenode 在内存中对元数据执行增删改操作</li><li>2NN 询问 nn 是否执行 checkpoint 操作，并带回 nn 的指令</li><li>当触发 cheekpoint 时，nn 中生成一个滚动日志文件（用于在记录后续的客户端请求），一个磁盘中编辑日志的复制文件</li><li>将磁盘中的镜像文件和复制好的编辑日志文件复制到 2NN 中</li><li>2NN 加载到内存并进行合并生成一个合并文件，将它拷贝到 nn 中</li><li>nn 将此文件覆盖磁盘中的原有文件（重命名的方式）</li></ul></li><li><p>fsimage</p><ul><li>hdfs 文件系统元数据的一个永久性检查点</li><li>hdfs oiv -p 文件类型 -i 镜像文件 -o 转换后文件输出路径</li></ul></li><li><p>edits</p><ul><li>存放 hdfs 文件系统所有的更新操作</li><li>hdfs oev -p 文件类型 -i 编辑日志 -o 转换后文件输出路径</li></ul></li><li><p>CheckPoint</p><ul><li><p>触发点</p><ul><li>1h</li><li>edits 数据满了，默认是执行了 1 百万次操作</li></ul></li><li><p>2nn 在 1min 内会到 nn 中确认 3 次操作次数</p></li></ul></li><li><p>NN 故障处理</p><ul><li>当 nn 故障时，可以将 2nn 中的数据拷贝到 nn 中，可恢复数据，存在丢失部分数据的情况，即时编辑日志信息不在 nn 中</li></ul></li><li><p>集群安全模式</p><ul><li><p>当启动 nn 中，nn 会加载镜像文件和编辑日志，同时等待所有的 dn 向 nn 进行注册块信息，此时 hdfs 处于安全模式</p></li><li><p>操作</p><ul><li>bin/hdfs dfsadmin -safemode get</li><li>bin/hdfs dfsadmin -safemode enter</li><li>bin/hdfs dfsadmin -safemode leave</li><li>bin/hdfs dfsadmin -safemode wait</li></ul></li></ul></li><li><p>NN 多目录</p><ul><li>nn 可在本地目录中配置多个，每个目录存储的数据相同</li></ul></li><li><p>dn 多目录</p><ul><li>dn 可在本地目录中配置多个，每个目录存储的数据不相同</li></ul></li></ul><h3 id="datanode"><a class="anchor" href="#datanode">#</a> datanode</h3><ul><li><p>机制</p><ul><li><p>一个数据块在 Dn 中存储的数据</p><ul><li>数据本身</li><li>数据长度</li><li>检验和</li><li>时间戳</li></ul></li><li><p>dn 启动以后会向 nn 进行注册</p></li><li><p>后续每间隔 1h 向 nn 报告包含的所有数据块信息</p></li><li><p>同时每 3s 与 nn 进行一次心跳，同时带回 nn 的指令</p></li><li><p>如果超过 10min + 30 s 未进行心跳时，nn 认为该节点不可用</p></li></ul></li><li><p>数据的完整性</p><ul><li>通过检验和来确保</li></ul></li><li><p>掉线时限参数设置</p><ul><li>TimeOut = 2 * dfs.namenode.heartbeat.recheck-interval + 10 * dfs.heartbeat.interval。</li></ul></li><li><p>服役新数据节点</p></li><li><p>退役旧数据节点</p><ul><li>白名单：确定允许访问 NameNode 的 DataNode 节点</li><li>黑名单：指定集群运行过程中退役 DataNode 节点</li><li>建议使用黑名单进行退役，黑明单退役相当于辞职，会将本节点的数据在 hdfs 进行转移，白名单相当于辞退，不会进行数据的转移</li></ul></li></ul><h2 id="mapreduce"><a class="anchor" href="#mapreduce">#</a> MapReduce</h2><h3 id="概论-3"><a class="anchor" href="#概论-3">#</a> 概论</h3><ul><li><p>MapReduce 定义</p><ul><li>是一个分布式运算程序的编程框架</li><li>核心功能：将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个 hadoop 集群上</li></ul></li><li><p>MapReduce 编程思想</p><ul><li>一个运算程序一般是一个 map 阶段和一个 reduce 阶段</li><li>MapTask 是并发执行，互不相干</li><li>ReduceTask 也是并发执行，互不相干，但是依赖于 MapTask 的输出结果</li></ul></li><li><p>MapReduce 进程</p><ul><li>mrAppmaster ： 负责整个程序过程调度和状态协调</li><li>MapTask ：负责 Map 阶段的整个数据处理</li><li>ReduceTask ：负责 Reduce 阶段的整个数据处理</li></ul></li><li><p>常用数据序列化类型</p><ul><li>String --&gt; Text</li><li>Map --&gt; MapWritable</li><li>Array --&gt; ArrayWritable</li></ul></li><li><p>MapReduce 编程规范</p><ul><li><p>Mapper 阶段</p><ul><li>用户自定义的 mapper 类要继承与 mapper () 类</li><li>指明 Mapper 的输入数据的 KV 对的数据类型</li><li>指明输出数据的 KV 对的数据类型</li><li>重写 map 方法</li><li>map () 方法，每一个 &lt; k,v&gt; 都会调用一次</li></ul></li><li><p>Reducer 阶段</p><ul><li>用户自定义的 Reducer 类要继承 Reducer 类</li><li>指明 Mapper 的输入数据的 KV 对的数据类型 ，与 mapper 输出的数据类型一致</li><li>指明输出数据的 KV 对的数据类型</li><li>重写 reduce () 方法</li><li>每一组数据相同的 k 的 &lt;k,v&gt; 会调用一些 reduce () 方法</li></ul></li><li><p>Driver 阶段</p><ul><li>提交整个 MR 程序到 YARN 集群上，提交的是封装好的 MapReduce 程序相关运行参数的 job 对象</li></ul></li></ul></li></ul></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i> </span><span class="text">更新于</span> <time title="修改时间：2021-09-29 23:08:53" itemprop="dateModified" datetime="2021-09-29T23:08:53+08:00">2021-09-29</time></span></div><div id="copyright"><ul><li class="author"><strong>本文作者： </strong>Miyazono <i class="ic i-at"><em>@</em></i>冬樱茶</li><li class="link"><strong>本文链接：</strong> <a href="https://github.com/Mayizono/miyazono.github.io/bigdata/hadoop/hadoop/" title="Hadoop 学习">https://github.com/Mayizono/miyazono.github.io/bigdata/hadoop/hadoop/</a></li><li class="license"><strong>版权声明： </strong>本站所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> 许可协议。转载请注明出处！</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/bigdata/kafka/Kafka%E6%80%BB%E7%BB%93/" itemprop="url" rel="prev" data-background-image="https:&#x2F;&#x2F;i3.wp.com&#x2F;wx4.sinaimg.cn&#x2F;mw690&#x2F;6833939bly1giclgi503lj20zk0m8hdt.jpg" title="Kafka学习"><span class="type">上一篇</span> <span class="category"><i class="ic i-flag"></i> Kafka</span><h3>Kafka学习</h3></a></div><div class="item right"><a href="/bigdata/hive/Hive%20%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="next" data-background-image="https:&#x2F;&#x2F;i3.wp.com&#x2F;wx4.sinaimg.cn&#x2F;mw690&#x2F;6833939bly1giclize41wj20zk0m87gk.jpg" title="Hive学习"><span class="type">下一篇</span> <span class="category"><i class="ic i-flag"></i> Hive</span><h3>Hive学习</h3></a></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#hadoop"><span class="toc-number">1.</span> <span class="toc-text">hadoop</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%A5%E9%97%A8"><span class="toc-number">1.1.</span> <span class="toc-text">入门</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A6%82%E8%AE%BA"><span class="toc-number">1.1.1.</span> <span class="toc-text">概论</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#hadoop%E6%A1%86%E6%9E%B6"><span class="toc-number">1.1.2.</span> <span class="toc-text">hadoop 框架</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#hadoop%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%BC%8F"><span class="toc-number">1.1.3.</span> <span class="toc-text">hadoop 运行模式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#hadoop3x%E4%B8%8E2x%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">1.1.4.</span> <span class="toc-text">hadoop3.x 与 2.x 的区别</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#hdfs"><span class="toc-number">1.2.</span> <span class="toc-text">HDFS</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A6%82%E8%AE%BA-2"><span class="toc-number">1.2.1.</span> <span class="toc-text">概论</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#hdfs%E4%B9%8Bshell%E6%93%8D%E4%BD%9C"><span class="toc-number">1.2.2.</span> <span class="toc-text">HDFS 之 shell 操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#hdfs%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%93%8D%E4%BD%9C"><span class="toc-number">1.2.3.</span> <span class="toc-text">hdfs 客户端操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#hdfs%E7%9A%84%E6%95%B0%E6%8D%AE%E6%B5%81"><span class="toc-number">1.2.4.</span> <span class="toc-text">HDFS 的数据流</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#nn%E5%92%8C2nn"><span class="toc-number">1.2.5.</span> <span class="toc-text">NN 和 2NN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#datanode"><span class="toc-number">1.2.6.</span> <span class="toc-text">datanode</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#mapreduce"><span class="toc-number">1.3.</span> <span class="toc-text">MapReduce</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A6%82%E8%AE%BA-3"><span class="toc-number">1.3.1.</span> <span class="toc-text">概论</span></a></li></ol></li></ol></li></ol></div><div class="related panel pjax" data-title="系列文章"><ul><li class="active"><a href="/bigdata/hadoop/hadoop/" rel="bookmark" title="Hadoop学习">Hadoop学习</a></li></ul></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="Miyazono" data-src="/images/avatar.jpg"><p class="name" itemprop="name">Miyazono</p><div class="description" itemprop="description"></div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">24</span> <span class="name">文章</span></a></div><div class="item categories"><a href="/categories/"><span class="count">13</span> <span class="name">分类</span></a></div></nav><div class="social"><a href="https://github.com/Mayizono" title="https:&#x2F;&#x2F;github.com&#x2F;Mayizono" class="item github"><i class="ic i-github"></i></a> <span class="exturl item music" data-url="aHR0cHM6Ly9tdXNpYy4xNjMuY29tLyMvdXNlci9ob21lP2lkPTEzNDMzNjQzOQ==" title="https:&#x2F;&#x2F;music.163.com&#x2F;#&#x2F;user&#x2F;home?id&#x3D;134336439"><i class="ic i-cloud-music"></i></span> <span class="exturl item weibo" data-url="aHR0cHM6Ly93ZWliby5jb20vdS8yOTAwNzY4Nzky" title="https:&#x2F;&#x2F;weibo.com&#x2F;u&#x2F;2900768792"><i class="ic i-weibo"></i></span> <span class="exturl item email" data-url="aHR0cHM6Ly94eW5zdHVkeUAxNjMuY29t" title="https:&#x2F;&#x2F;xynstudy@163.com"><i class="ic i-envelope"></i></span></div><ul class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-feather"></i>文章</a><ul class="submenu"><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a></li><li class="item"><a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a></li><li class="item"><a href="/tags/" rel="section"><i class="ic i-tags"></i>标签</a></li></ul></li><li class="item"><a href="/friends/" rel="section"><i class="ic i-heart"></i>friends</a></li><li class="item"><a href="/mikutap/" rel="section"><i class="ic i-star"></i>mikutap</a></li><li class="item"><a href="/time" rel="section"><i class="ic i-heart"></i>time</a></li></ul></div></div></div><ul id="quick"><li class="prev pjax"><a href="/bigdata/kafka/Kafka%E6%80%BB%E7%BB%93/" rel="prev" title="上一篇"><i class="ic i-chevron-left"></i></a></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/bigdata/hive/Hive%20%E5%AD%A6%E4%B9%A0/" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>随机文章</h2><ul><li class="item"><div class="breadcrumb"><a href="/categories/bigdata/" title="分类于 大数据">大数据</a> <i class="ic i-angle-right"></i> <a href="/categories/bigdata/flink/" title="分类于 Flink">Flink</a></div><span><a href="/bigdata/flink/Flink%E5%AE%9E%E6%88%98%E8%AE%A1%E7%AE%97%E7%83%AD%E9%97%A8%E5%95%86%E5%93%81/" title="Flink实战计算热门商品">Flink实战计算热门商品</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/bigdata/" title="分类于 大数据">大数据</a> <i class="ic i-angle-right"></i> <a href="/categories/bigdata/hive/" title="分类于 Hive">Hive</a></div><span><a href="/bigdata/hive/leetcode%E6%95%B0%E6%8D%AE%E5%BA%93%E9%A2%98%E7%9B%AE1-123%E9%A2%98%EF%BC%8820-08-14)/" title="力扣数据库">力扣数据库</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/kawaii/" title="分类于 可爱女魔头">可爱女魔头</a> <i class="ic i-angle-right"></i> <a href="/categories/kawaii/moments/" title="分类于 Moments">Moments</a></div><span><a href="/kawaii/moments/%E5%8F%AF%E7%88%B1%E5%A5%B3%E9%AD%94%E5%A4%B4/" title="可爱女魔头">可爱女魔头</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/bigdata/" title="分类于 大数据">大数据</a> <i class="ic i-angle-right"></i> <a href="/categories/bigdata/pro/" title="分类于 大厂学习">大厂学习</a></div><span><a href="/bigdata/pro/%E5%94%AF%E5%93%81%E4%BC%9A%E4%BA%BF%E7%BA%A7%E6%95%B0%E6%8D%AE%E6%9C%8D%E5%8A%A1%E5%B9%B3%E5%8F%B0%E5%AE%9E%E8%B7%B5/" title="唯品会亿级数据服务平台实践">唯品会亿级数据服务平台实践</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/bigdata/" title="分类于 大数据">大数据</a> <i class="ic i-angle-right"></i> <a href="/categories/bigdata/flume/" title="分类于 Flume">Flume</a></div><span><a href="/bigdata/flume/Flume%E7%BB%83%E4%B9%A0/" title="Flume练习题">Flume练习题</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/bigdata/" title="分类于 大数据">大数据</a> <i class="ic i-angle-right"></i> <a href="/categories/bigdata/hadoop/" title="分类于 Hadoop">Hadoop</a></div><span><a href="/bigdata/hadoop/hadoop/" title="Hadoop学习">Hadoop学习</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/bigdata/" title="分类于 大数据">大数据</a> <i class="ic i-angle-right"></i> <a href="/categories/bigdata/flink/" title="分类于 Flink">Flink</a></div><span><a href="/bigdata/flink/Flink%E7%AE%A1%E7%90%86Kafka%E7%9A%84%E6%B6%88%E8%B4%B9%E4%BD%8D%E7%82%B9/" title="Flink管理 Kafka 消费位点">Flink管理 Kafka 消费位点</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/bigdata/" title="分类于 大数据">大数据</a> <i class="ic i-angle-right"></i> <a href="/categories/bigdata/zookeeper/" title="分类于 Zookeeper">Zookeeper</a></div><span><a href="/bigdata/zookeeper/zookeeper%E6%80%BB%E7%BB%93/" title="zookeeper总结">zookeeper总结</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/bigdata/" title="分类于 大数据">大数据</a> <i class="ic i-angle-right"></i> <a href="/categories/bigdata/hive/" title="分类于 Hive">Hive</a></div><span><a href="/bigdata/hive/HQL%E7%BB%83%E4%B9%A0%E9%A2%98/" title="HQL练习题">HQL练习题</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/bigdata/" title="分类于 大数据">大数据</a> <i class="ic i-angle-right"></i> <a href="/categories/bigdata/pgsql/" title="分类于 PostgreSQL">PostgreSQL</a></div><span><a href="/bigdata/pgsql/Postgresql%E5%AD%A6%E4%B9%A0/" title="PgSql学习">PgSql学习</a></span></li></ul></div><div><h2>最新评论</h2><ul class="leancloud-recent-comment"></ul></div></div><div class="status"><div class="copyright">&copy; 2018 – <span itemprop="copyrightYear">2025</span> <span class="with-love"><i class="ic i-sakura rotate"></i> </span><span class="author" itemprop="copyrightHolder">Miyazono @ Yume Shoka</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i> </span><span title="站点总字数">311k 字</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="ic i-coffee"></i> </span><span title="站点阅读时长">4:43</span></div><div class="powered-by">基于 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<a href="https://github.com/amehime/hexo-theme-shoka">Shoka</a></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"bigdata/hadoop/hadoop/",favicon:{show:"（●´3｀●）やれやれだぜ",hide:"(´Д｀)大変だ！"},search:{placeholder:"文章搜索",empty:"关于 「 ${query} 」，什么也没搜到",stats:"${time} ms 内找到 ${hits} 条结果"},valine:!0,fancybox:!0,copyright:'复制成功，转载请遵守 <i class="ic i-creative-commons"></i>BY-NC-SA 协议。',ignores:[function(e){return e.includes("#")},function(e){return new RegExp(LOCAL.path+"$").test(e)}]}</script><script src="https://cdn.polyfill.io/v2/polyfill.js"></script><script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script><script src="/js/app.js?v=0.2.5"></script></body></html><!-- rebuild by hrmmi -->