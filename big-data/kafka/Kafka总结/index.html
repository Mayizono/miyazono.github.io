<!-- build time:Tue Oct 26 2021 13:34:05 GMT+0800 (中国标准时间) --><!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="alternate" type="application/rss+xml" title="冬樱茶" href="https://github.com/Mayizono/miyazono.github.io/rss.xml"><link rel="alternate" type="application/atom+xml" title="冬樱茶" href="https://github.com/Mayizono/miyazono.github.io/atom.xml"><link rel="alternate" type="application/json" title="冬樱茶" href="https://github.com/Mayizono/miyazono.github.io/feed.json"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/css/app.css?v=0.2.5"><link rel="canonical" href="https://github.com/Mayizono/miyazono.github.io/big-data/kafka/Kafka%E6%80%BB%E7%BB%93/"><title>Kafka 学习 - Kafka - 大数据 | Yume Shoka = 冬樱茶</title><meta name="generator" content="Hexo 5.4.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">Kafka 学习</h1><div class="meta"><span class="item" title="创建时间：2019-01-12 00:00:00"><span class="icon"><i class="ic i-calendar"></i> </span><span class="text">发表于</span> <time itemprop="dateCreated datePublished" datetime="2019-01-12T00:00:00+08:00">2019-01-12</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span class="text">本文字数</span> <span>22k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span class="text">阅读时长</span> <span>20 分钟</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">Yume Shoka</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div id="imgs" class="pjax"><ul><li class="item" data-background-image="https://tva4.sinaimg.cn/large/6833939bly1giclj9410cj20zk0m8h12.jpg"></li><li class="item" data-background-image="https://tva4.sinaimg.cn/large/6833939bly1gipesx5fdwj20zk0m81kx.jpg"></li><li class="item" data-background-image="https://tva4.sinaimg.cn/large/6833939bly1gipewkhf1zj20zk0m81kx.jpg"></li><li class="item" data-background-image="https://tva4.sinaimg.cn/large/6833939bly1giph4lm9i7j20zk0m84qp.jpg"></li><li class="item" data-background-image="https://tva4.sinaimg.cn/large/6833939bly1gicljgocqbj20zk0m8e81.jpg"></li><li class="item" data-background-image="https://tva4.sinaimg.cn/large/6833939bly1giclj61ylzj20zk0m8b29.jpg"></li></ul></div><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div></header><main><div class="inner"><div id="main" class="pjax"><div class="article wrap"><div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i> <span><a href="/">首页</a></span><i class="ic i-angle-right"></i> <span itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/big-data/" itemprop="item" rel="index" title="分类于 大数据"><span itemprop="name">大数据</span></a><meta itemprop="position" content="1"></span><i class="ic i-angle-right"></i> <span class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/big-data/kafka/" itemprop="item" rel="index" title="分类于 Kafka"><span itemprop="name">Kafka</span></a><meta itemprop="position" content="2"></span></div><article itemscope itemtype="http://schema.org/Article" class="post block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://github.com/Mayizono/miyazono.github.io/big-data/kafka/Kafka%E6%80%BB%E7%BB%93/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="Miyazono"><meta itemprop="description" content=", "></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="冬樱茶"></span><div class="body md" itemprop="articleBody"><h1 id="kafka总结"><a class="anchor" href="#kafka总结">#</a> Kafka 总结</h1><h2 id="一-kafka概述"><a class="anchor" href="#一-kafka概述">#</a> 一、kafka 概述</h2><h3 id="11-kafka定义"><a class="anchor" href="#11-kafka定义">#</a> 1.1 kafka 定义</h3><blockquote><p>Kafka 是一个分布式的基于<strong>发布 / 订阅</strong>模式的<strong>消息队列，<strong>主要应用于大数据</strong>实时</strong>处理领域。</p><p>订阅式模式：一对多的关系，一个生产者，数据存储在消息队列中，多个消费者均可从这个消息对列中获取数据，<strong>消费者消费数据之后不会清除消息。</strong></p></blockquote><h3 id="12-框架说明"><a class="anchor" href="#12-框架说明">#</a> 1.2 框架说明</h3><blockquote><p>一般都是从命令行和 API 两个方面进行讲解。</p><p>数据处理框架需要从数据的安全性以及效率两个方面深入了解。</p></blockquote><h3 id="13-kafka涉及的关键词"><a class="anchor" href="#13-kafka涉及的关键词">#</a> 1.3 Kafka 涉及的关键词</h3><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token number">1.</span> producer: 消息的生产者，即为向kafka broker发消息<span class="token punctuation">;</span></pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token number">2.</span> broker ： kafka集群的节点；</pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token number">3.</span> topic : 队列（话题），生产者和消费者面向的都是一个topic；</pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token number">4.</span> message：消息，队列中的一条消息；</pre></td></tr><tr><td data-num="5"></td><td><pre><span class="token number">5.</span> <span class="token keyword">partition</span>: 分区，为方便扩展和提高吞吐量，将一个topic分为了多个<span class="token keyword">partition</span>；</pre></td></tr><tr><td data-num="6"></td><td><pre><span class="token number">6.</span> <span class="token keyword">index</span> ： 消息数据在log文件中的索引；</pre></td></tr><tr><td data-num="7"></td><td><pre><span class="token number">7.</span> log ：消息的具体数据；</pre></td></tr><tr><td data-num="8"></td><td><pre><span class="token number">8.</span> timeindex： 时间索引，代表发送的数据时间索引；</pre></td></tr><tr><td data-num="9"></td><td><pre><span class="token number">9.</span> <span class="token keyword">offset</span> ： 消息的偏移量，每一条消息都对应一个<span class="token keyword">offset</span>；</pre></td></tr><tr><td data-num="10"></td><td><pre><span class="token number">10.</span> segment : 一个分片数据；</pre></td></tr><tr><td data-num="11"></td><td><pre><span class="token number">11.</span> leader ：每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是leader；</pre></td></tr><tr><td data-num="12"></td><td><pre><span class="token number">12.</span> follower : 每个分区多个副本中的“从”，实时从leader中同步数据，保持和leader数据的同步。leader发生故障时，某个follower会成为新的leader</pre></td></tr></table></figure><h2 id="二-kafka安装"><a class="anchor" href="#二-kafka安装">#</a> 二、Kafka 安装</h2><h3 id="21-集群部署"><a class="anchor" href="#21-集群部署">#</a> 2.1 集群部署</h3><h4 id="221-解压安装包"><a class="anchor" href="#221-解压安装包">#</a> 2.2.1 解压安装包</h4><p>在 /opt/software 目录下</p><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre>tar <span class="token operator">-</span>zxvf kafka_2<span class="token punctuation">.</span><span class="token number">11</span><span class="token operator">-</span><span class="token number">2.4</span><span class="token number">.1</span><span class="token punctuation">.</span>tgz <span class="token operator">-</span>C <span class="token operator">/</span>opt<span class="token operator">/</span>module<span class="token operator">/</span></pre></td></tr></table></figure><h4 id="222-修改解压后的文件名称"><a class="anchor" href="#222-修改解压后的文件名称">#</a> 2.2.2 修改解压后的文件名称</h4><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre>mv kafka_2<span class="token punctuation">.</span><span class="token number">11</span><span class="token operator">-</span><span class="token number">2.4</span><span class="token number">.1</span><span class="token operator">/</span> kafka</pre></td></tr></table></figure><h4 id="223-创建logs文件夹"><a class="anchor" href="#223-创建logs文件夹">#</a> 2.2.3 创建 logs 文件夹</h4><p>在 /opt/module/kafka 目录下</p><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre>mkdir logs</pre></td></tr></table></figure><h4 id="224-修改配置文件"><a class="anchor" href="#224-修改配置文件">#</a> 2.2.4 修改配置文件</h4><p>/opt/module/kafk 路径下</p><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre>vim config<span class="token operator">/</span>server<span class="token punctuation">.</span>properties</pre></td></tr></table></figure><p>修改如下三个参数，修改后的值如下：</p><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre>broker<span class="token punctuation">.</span>id<span class="token operator">=</span><span class="token number">2</span>；</pre></td></tr><tr><td data-num="2"></td><td><pre>log<span class="token punctuation">.</span>dirs<span class="token operator">=</span><span class="token operator">/</span>opt<span class="token operator">/</span>module<span class="token operator">/</span>kafka<span class="token operator">/</span>logs</pre></td></tr><tr><td data-num="3"></td><td><pre>zookeeper<span class="token punctuation">.</span><span class="token keyword">connect</span><span class="token operator">=</span>hadoop102:<span class="token number">2181</span><span class="token punctuation">,</span>hadoop103:<span class="token number">2181</span><span class="token punctuation">,</span>hadoop104:<span class="token number">2181</span><span class="token operator">/</span>kafka</pre></td></tr></table></figure><h4 id="225-配置环境变量"><a class="anchor" href="#225-配置环境变量">#</a> 2.2.5 配置环境变量</h4><pre><code class="language-Sql">sudo vim /etc/profile.d/my_env.sh
</code></pre><p>增加如下配置：</p><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token comment">#KAFKA_HOME</span></pre></td></tr><tr><td data-num="2"></td><td><pre>KAFKA_HOME<span class="token operator">=</span><span class="token operator">/</span>opt<span class="token operator">/</span>module<span class="token operator">/</span>kafka</pre></td></tr><tr><td data-num="3"></td><td><pre>PATH<span class="token operator">=</span>$PATH:$KAFKA_HOME<span class="token operator">/</span>bin</pre></td></tr><tr><td data-num="4"></td><td><pre>export KAFKA_HOME</pre></td></tr></table></figure><p>生效配置文件：</p><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre>source <span class="token operator">/</span>etc<span class="token operator">/</span>profile</pre></td></tr></table></figure><h4 id="226-分发安装包"><a class="anchor" href="#226-分发安装包">#</a> 2.2.6 分发安装包</h4><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre>xsync kafka<span class="token operator">/</span></pre></td></tr></table></figure><p>注意：分发之后记得配置其他机器的环境变量</p><h4 id="227-修改其他机器的配置文件"><a class="anchor" href="#227-修改其他机器的配置文件">#</a> 2.2.7 修改其他机器的配置文件</h4><ul><li><p>/opt/module/kafka/config/server.properties 中的 broker.id=3、broker.id=4</p><p>注：broker.id 不得重复</p></li></ul><h4 id="228-启动集群"><a class="anchor" href="#228-启动集群">#</a> 2.2.8 启动集群</h4><ol><li><p>首先启动 zookeeper 集群和 hadoop 集群</p></li><li><p>依次在 hadoop102、hadoop103、hadoop104 节点上启动 kafka</p></li></ol><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre>kafka<span class="token operator">-</span>server<span class="token operator">-</span><span class="token keyword">start</span><span class="token punctuation">.</span>sh <span class="token operator">-</span>daemon $KAFKA_HOME<span class="token operator">/</span>config<span class="token operator">/</span>server<span class="token punctuation">.</span>properties</pre></td></tr></table></figure><ul><li>-daemon 属于后台启动，没有 - daemon 则为前台启动</li></ul><h4 id="229-关闭集群"><a class="anchor" href="#229-关闭集群">#</a> 2.2.9 关闭集群</h4><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre>kafka<span class="token operator">-</span>server<span class="token operator">-</span>stop<span class="token punctuation">.</span>sh</pre></td></tr></table></figure><h4 id="2210-kafka群起群停脚本"><a class="anchor" href="#2210-kafka群起群停脚本">#</a> 2.2.10 kafka 群起群停脚本</h4><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token comment">#！bin/bash</span></pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">if</span> <span class="token punctuation">[</span> $<span class="token comment"># -lt 1 ]</span></pre></td></tr><tr><td data-num="3"></td><td><pre> <span class="token keyword">then</span></pre></td></tr><tr><td data-num="4"></td><td><pre>   echo <span class="token string">"No Args Input Error"</span></pre></td></tr><tr><td data-num="5"></td><td><pre>   <span class="token keyword">exit</span></pre></td></tr><tr><td data-num="6"></td><td><pre>fi</pre></td></tr><tr><td data-num="7"></td><td><pre><span class="token keyword">case</span> $<span class="token number">1</span> <span class="token operator">in</span></pre></td></tr><tr><td data-num="8"></td><td><pre><span class="token string">"start"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="9"></td><td><pre><span class="token keyword">for</span> i <span class="token operator">in</span> <span class="token punctuation">`</span>cat <span class="token operator">/</span>opt<span class="token operator">/</span>module<span class="token operator">/</span>hadoop<span class="token operator">-</span><span class="token number">3.1</span><span class="token number">.3</span><span class="token operator">/</span>etc<span class="token operator">/</span>hadoop<span class="token operator">/</span>workers<span class="token punctuation">`</span></pre></td></tr><tr><td data-num="10"></td><td><pre><span class="token keyword">do</span></pre></td></tr><tr><td data-num="11"></td><td><pre>echo <span class="token string">"==========start $i kafka=========="</span></pre></td></tr><tr><td data-num="12"></td><td><pre>ssh $i <span class="token string">'$KAFKA_HOME/bin/kafka-server-start.sh -daemon /opt/module/kafka/config/server.properties'</span></pre></td></tr><tr><td data-num="13"></td><td><pre>done</pre></td></tr><tr><td data-num="14"></td><td><pre><span class="token punctuation">;</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="15"></td><td><pre><span class="token string">"stop"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="16"></td><td><pre><span class="token keyword">for</span> i <span class="token operator">in</span> <span class="token punctuation">`</span>cat <span class="token operator">/</span>opt<span class="token operator">/</span>module<span class="token operator">/</span>hadoop<span class="token operator">-</span><span class="token number">3.1</span><span class="token number">.3</span><span class="token operator">/</span>etc<span class="token operator">/</span>hadoop<span class="token operator">/</span>workers<span class="token punctuation">`</span></pre></td></tr><tr><td data-num="17"></td><td><pre><span class="token keyword">do</span></pre></td></tr><tr><td data-num="18"></td><td><pre>echo <span class="token string">"==========stop $i kafka=========="</span></pre></td></tr><tr><td data-num="19"></td><td><pre>ssh $i <span class="token string">'$KAFKA_HOME/bin/kafka-server-stop.sh'</span></pre></td></tr><tr><td data-num="20"></td><td><pre>done</pre></td></tr><tr><td data-num="21"></td><td><pre><span class="token punctuation">;</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="22"></td><td><pre><span class="token operator">*</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="23"></td><td><pre>  echo <span class="token string">"Input Args Error"</span></pre></td></tr><tr><td data-num="24"></td><td><pre><span class="token punctuation">;</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="25"></td><td><pre>esac</pre></td></tr></table></figure><h3 id="22-kafka命令操作"><a class="anchor" href="#22-kafka命令操作">#</a> 2.2 Kafka 命令操作</h3><h4 id="221-查看当前服务器中的所有topic"><a class="anchor" href="#221-查看当前服务器中的所有topic">#</a> 2.2.1 查看当前服务器中的所有 topic</h4><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre>kafka<span class="token operator">-</span>topics<span class="token punctuation">.</span>sh  <span class="token comment">--bootstrap-server hadoop102:9092 --list</span></pre></td></tr></table></figure><p>选项说明：</p><ul><li>--list ：查看 kafka 所有的 topic</li><li>--bootstrap-server : 连接 kafka 集群</li><li>--hadoop102:9092：hadoop102 是指连接 kafka 任意一台机器，9092：kafka 内部通信的端口</li></ul><h4 id="222-创建topic"><a class="anchor" href="#222-创建topic">#</a> 2.2.2 创建 topic</h4><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre>kafka<span class="token operator">-</span>topics<span class="token punctuation">.</span>sh  <span class="token comment">--bootstrap-server hadoop102:9092 --create --topic first --partitions 2 --replication-factor 2</span></pre></td></tr></table></figure><p>选项说明：</p><ul><li>--topic : 定义 topic 名字</li><li>--partitions : 定义分区数</li><li>--replication-factor : 定义副本数</li></ul><h4 id="223-查看某个topic的详情"><a class="anchor" href="#223-查看某个topic的详情">#</a> 2.2.3 查看某个 Topic 的详情</h4><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre>kafka<span class="token operator">-</span>topics<span class="token punctuation">.</span>sh <span class="token comment">--bootstrap-server hadoop102:9092 --describe --topic first</span></pre></td></tr></table></figure><p>选项说明：</p><ul><li>--topic first ： 查看指定的话题，如果不加此选项，则表示查看所有的话题</li></ul><h4 id="224-修改分区数"><a class="anchor" href="#224-修改分区数">#</a> 2.2.4 修改分区数</h4><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre>kafka<span class="token operator">-</span>topics<span class="token punctuation">.</span>sh <span class="token comment">--bootstrap-server hadoop102:9092 --alter --topic first --partitions 6</span></pre></td></tr></table></figure><p>说明：</p><ul><li>-- 分区数只能增加不能减少</li><li>分区内部消息有序，分区之间消息无序</li></ul><h4 id="225-发送消息"><a class="anchor" href="#225-发送消息">#</a> 2.2.5 发送消息</h4><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre>kafka<span class="token operator">-</span>console<span class="token operator">-</span>producer<span class="token punctuation">.</span>sh <span class="token comment">--broker-list  hadoop102:9092,hadoop103:9092,hadoop104:9092 --topic first</span></pre></td></tr><tr><td data-num="2"></td><td><pre></pre></td></tr><tr><td data-num="3"></td><td><pre></pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token operator">></span>hello world</pre></td></tr><tr><td data-num="5"></td><td><pre><span class="token operator">></span>miyazono  miyazono</pre></td></tr></table></figure><p>选项说明：</p><ul><li>hadoop102:9092,hadoop103:9092,hadoop104:9092 : kafka 的集群中的 broker，其实写一个也是可以的，写 3 个的目的只是避免当连接的 kafka 集群 broker 故障时连不上 kafka 集群的情况。</li></ul><h4 id="226-消费消息"><a class="anchor" href="#226-消费消息">#</a> 2.2.6 消费消息</h4><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre>kafka<span class="token operator">-</span>console<span class="token operator">-</span>consumer<span class="token punctuation">.</span>sh <span class="token comment">--bootstrap-server hadoop102:9092 --from-beginning --topic first</span></pre></td></tr></table></figure><p>选项说明：</p><ul><li><p>--from-beginning ：</p><p>加上：会把 topic 中以往所有的数据都读取出来</p><p>不加：此时只会消费最新的数据，原来 topic 中的数据不会被消费</p></li></ul><h4 id="227-删除topic"><a class="anchor" href="#227-删除topic">#</a> 2.2.7 删除 topic</h4><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre>kafka<span class="token operator">-</span>topics<span class="token punctuation">.</span>sh <span class="token comment">--bootstrap-server hadoop102:9820 --delete --topic first</span></pre></td></tr></table></figure><h2 id="三-kafka深入流程"><a class="anchor" href="#三-kafka深入流程">#</a> 三、 Kafka 深入流程</h2><p>说明：此框架步步引导，采取提出问题解决问题的方式阐述。</p><h3 id="31-kafka工作流程及文件存储机制"><a class="anchor" href="#31-kafka工作流程及文件存储机制">#</a> 3.1 Kafka 工作流程及文件存储机制</h3><p><img data-src="https://miyazono-1255488789.cos.ap-shanghai.myqcloud.com/markdown/20210822155602.png" alt="图2"></p><ol><li>kafka 以 topic（话题）为单位，每一个话题分为多个区（创建话题的时候指定分区的个数），每个分区中存储的数据是不一样的，同时每个分区的数据在其他分区也会创建副本。</li><li>不同的分区分布在 kafka 集群不同的机器（broker，代理人）上面；</li><li>消息的生产和消费均是分区为单位；</li><li>分区内的数据是有序的，分区之间的顺序是无序的；</li><li>offset 指消息的偏移量；</li><li>每个分区都是一个文件夹，文件中包含 index（数据在 log 中的索引）、log（真实的数据）、timeindex (数据发送的时间索引) ，时间索引和 index 索引均是用来提高查询数据效率；</li><li>当产生新的数据以后会向 log 文件中进行追加，同时 index 和 timeindex 也会增加；</li><li>Kafka 采取了<strong>分片</strong>和<strong>索引</strong>机制。</li><li>topic 是逻辑上的概念，而 partition 是物理上的概念，每个 partition 对应于一个 log 文件，该 log 文件中存储的就是 producer 生产的数据。Producer 生产的数据会被不断追加到该 log 文件末端，且每条数据都有自己的 offset。消费者组中的每个消费者，都会实时记录自己消费到了哪个 offset，以便出错恢复时，从上次的位置继续消费</li></ol><p><img data-src="https://miyazono-1255488789.cos.ap-shanghai.myqcloud.com/markdown/20210822155644.png" alt="图1"></p><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token comment">-- 灵魂拷问 1：</span></pre></td></tr><tr><td data-num="2"></td><td><pre>产生的数据一直向同一个log中进行追加，会有什么问题呢？</pre></td></tr><tr><td data-num="3"></td><td><pre></pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token comment">-- 答案：</span></pre></td></tr><tr><td data-num="5"></td><td><pre>log中的数据会越来越大，查询和读取效率会变慢。</pre></td></tr><tr><td data-num="6"></td><td><pre></pre></td></tr><tr><td data-num="7"></td><td><pre><span class="token comment">-- 解决方式：</span></pre></td></tr><tr><td data-num="8"></td><td><pre>数据达到一定程度以后（默认值为<span class="token number">1</span>G：log<span class="token punctuation">.</span>segment<span class="token punctuation">.</span>bytes <span class="token operator">=</span> <span class="token number">1</span>G），log会进行数据切分，生成多个segment切分文件。</pre></td></tr><tr><td data-num="9"></td><td><pre>切分后的文件依然包含<span class="token keyword">index</span>、log、timeindex 。所以三个文件是作为一个整体的。 <span class="token comment">-- 切分机制</span></pre></td></tr></table></figure><p><img data-src="https://miyazono-1255488789.cos.ap-shanghai.myqcloud.com/markdown/20210822155702.png" alt="image-20200714205920775"></p><p><img data-src="https://miyazono-1255488789.cos.ap-shanghai.myqcloud.com/markdown/20210822155724.png" alt="image-20200714210619003"></p><blockquote><p>切分的文件位于同一个文件夹下，该文件夹的命名规则为：<mark>topic 名称 + 分区序号</mark>。</p><p>例如，first 这个 topic 有三个分区，则其对应的文件夹为 first-0,first-1,first-2</p></blockquote><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token comment">-- 灵魂拷问 2：</span></pre></td></tr><tr><td data-num="2"></td><td><pre>现在假如有两个切分的文件，当有一个消费者需要消费一条消息（假如是 <span class="token keyword">offset</span> <span class="token operator">=</span> <span class="token number">3</span>），怎么知道这个消息在哪个切分文件中，以及真实数据如何查询？</pre></td></tr><tr><td data-num="3"></td><td><pre></pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token comment">-- 答案：</span></pre></td></tr><tr><td data-num="5"></td><td><pre><span class="token number">1</span>）log和<span class="token keyword">index</span>文件名说明： <span class="token comment">-- 牢记 log、index、timeindex 是一个整体</span></pre></td></tr><tr><td data-num="6"></td><td><pre><span class="token keyword">index</span>：<span class="token number">00000000000000000000.</span><span class="token keyword">index</span></pre></td></tr><tr><td data-num="7"></td><td><pre>log：<span class="token number">00000000000000000000.</span>log</pre></td></tr><tr><td data-num="8"></td><td><pre>前面的数字<span class="token number">00000000000000000000</span>：代表此log文件中第一条消息的<span class="token keyword">offset</span>。</pre></td></tr><tr><td data-num="9"></td><td><pre><span class="token string">'.index文件存储大量的索引信息，“.log”文件存储大量的数据，索引文件中的元数据指向对应数据文件中message的物理偏移地址'</span></pre></td></tr><tr><td data-num="10"></td><td><pre><span class="token number">2</span>） 查询的方式：</pre></td></tr><tr><td data-num="11"></td><td><pre>根据消费消息的<span class="token keyword">offset</span>值 <span class="token comment">--> 找到指定的 index 文件 --> 匹配此条消息在 log 文件中数据的偏移量（即该数据在 log 文件中起始位置）--> 找到待消费的数据</span></pre></td></tr></table></figure><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token comment">-- 灵魂拷问 3：</span></pre></td></tr><tr><td data-num="2"></td><td><pre>为什么kafka要采取向一个log文件中追加数据呢？</pre></td></tr><tr><td data-num="3"></td><td><pre></pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token comment">-- 答案：</span></pre></td></tr><tr><td data-num="5"></td><td><pre><span class="token number">1</span>）减少IO；</pre></td></tr><tr><td data-num="6"></td><td><pre><span class="token number">2</span>）消费数据是连续进行消费，连续读取数据的效率高。</pre></td></tr></table></figure><h3 id="32-kafka之生产者producer"><a class="anchor" href="#32-kafka之生产者producer">#</a> 3.2 Kafka 之生产者 producer</h3><h4 id="321-分区策略"><a class="anchor" href="#321-分区策略">#</a> 3.2.1 分区策略</h4><ol><li>首先 producer 发送的数据会被封装成 ProducerRecord 对象，根据对象的参数，分区情况如下：<ul><li><p>情况 1 ：指定了 partition；</p></li><li><p>情况 2 ：未指定 partition，封装 key，则按照 key 的 hashcode % 分区数量 得出在哪个分区；</p></li><li><p>情况 3：未指定 partition，也未封装 key 处理方式 :</p><p>参数 1：producer 发送的数据量：batch.size，默认值为 16Kb；</p><p>条件 2：<span class="exturl" data-url="aHR0cDovL2xpbmdlci5tcw==">linger.ms</span>：两条数据发送的间隔时间 t ，默认值为 0s；</p><p>当发送的数据量 &lt; batch.size 并且 发送的数据时间间隔 &lt; t 时，所有的数据在一个分区；</p><p>当发送的数据量 &gt; batch.size 或者 发送的数据时间间隔 &gt; t 时，则数据会进入下一个分区；</p><p>分区与分区之间采取轮询的方式。</p><p><img data-src="https://miyazono-1255488789.cos.ap-shanghai.myqcloud.com/markdown/20210822155751.png" alt="图4"></p><p><img data-src="https://miyazono-1255488789.cos.ap-shanghai.myqcloud.com/markdown/20210822155800.png" alt="图3"></p></li></ul></li></ol><h4 id="322-数据可靠性保证"><a class="anchor" href="#322-数据可靠性保证">#</a> 3.2.2 数据可靠性保证</h4><p>数据传输流程：</p><p>producer -----&gt; server（kafka） ---------&gt; 消费者</p><ul><li>过程 1：producer -----&gt; server（kafka）</li></ul><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token comment">-- 灵魂拷问 1： </span></pre></td></tr><tr><td data-num="2"></td><td><pre>如何保证从producer发送数据server的过程中数据不丢失？</pre></td></tr><tr><td data-num="3"></td><td><pre></pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token comment">-- 答案：</span></pre></td></tr><tr><td data-num="5"></td><td><pre>server收到数据以后会回执，发送ack（acknowledgement确认收到）给producer，producer收到ack以后，则确定数据传送的过程中没有丢失。</pre></td></tr></table></figure><ul><li>过程 2 ： server 的数据存储过程</li></ul><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token comment">-- 灵魂拷问 2：</span></pre></td></tr><tr><td data-num="2"></td><td><pre>如何确保数据在server中能够被妥善保管呢？</pre></td></tr><tr><td data-num="3"></td><td><pre></pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token comment">-- 答案：</span></pre></td></tr><tr><td data-num="5"></td><td><pre>server向producer回执ack的时机：</pre></td></tr><tr><td data-num="6"></td><td><pre>模式<span class="token number">1</span>：leader收到消息以后立即回复ack；</pre></td></tr><tr><td data-num="7"></td><td><pre>模式<span class="token number">2</span>：leader收到消息并存储在本地以后，立即回复ack；</pre></td></tr><tr><td data-num="8"></td><td><pre>模式<span class="token number">3</span>：leader收到消息后，所有follow从leader中拉取数据，当所有的follower完成存储以后，leader向producer回复ack。</pre></td></tr><tr><td data-num="9"></td><td><pre></pre></td></tr><tr><td data-num="10"></td><td><pre>说明：情况<span class="token number">1</span><span class="token operator">/</span><span class="token number">2</span><span class="token operator">/</span><span class="token number">3</span>是通过acks参数进行配置。</pre></td></tr><tr><td data-num="11"></td><td><pre>acks<span class="token operator">=</span><span class="token number">0</span> <span class="token comment">-- leader 收到消息以后立即回复 ack</span></pre></td></tr><tr><td data-num="12"></td><td><pre>acks<span class="token operator">=</span><span class="token number">1</span> <span class="token comment">-- leader 收到消息并存储在本地以后，立即回复 ack</span></pre></td></tr><tr><td data-num="13"></td><td><pre>acks<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>或<span class="token keyword">all</span> <span class="token comment">--leader 收到消息后，所有 follow 从 leader 中拉取数据，当所有的 follower 完成存储以后，leader 向 producer 回复 ack</span></pre></td></tr><tr><td data-num="14"></td><td><pre><span class="token comment">-- 默认情况下是 acks=1；</span></pre></td></tr><tr><td data-num="15"></td><td><pre></pre></td></tr><tr><td data-num="16"></td><td><pre><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span></pre></td></tr><tr><td data-num="17"></td><td><pre><span class="token comment">-- 灵魂拷问 3：</span></pre></td></tr><tr><td data-num="18"></td><td><pre>leader与follower副本数据同步策略是什么呢？</pre></td></tr><tr><td data-num="19"></td><td><pre></pre></td></tr><tr><td data-num="20"></td><td><pre><span class="token comment">-- 答案</span></pre></td></tr><tr><td data-num="21"></td><td><pre>两种副本同步策略。</pre></td></tr><tr><td data-num="22"></td><td><pre>第一种：半数以上完成同步，就发送ack</pre></td></tr><tr><td data-num="23"></td><td><pre>第二种：全部完成同步，才发送ack</pre></td></tr><tr><td data-num="24"></td><td><pre></pre></td></tr><tr><td data-num="25"></td><td><pre><span class="token string">'kafka选择全部完成同步，才发送ack'</span></pre></td></tr><tr><td data-num="26"></td><td><pre></pre></td></tr><tr><td data-num="27"></td><td><pre><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span><span class="token operator">=</span></pre></td></tr><tr><td data-num="28"></td><td><pre><span class="token comment">-- 灵魂拷问 4：</span></pre></td></tr><tr><td data-num="29"></td><td><pre>kafka选择第二种副本同步策略会有哪些问题呢？</pre></td></tr><tr><td data-num="30"></td><td><pre></pre></td></tr><tr><td data-num="31"></td><td><pre><span class="token comment">-- 答案：</span></pre></td></tr><tr><td data-num="32"></td><td><pre>问题<span class="token number">1</span>：follower同步leader的数据时，当某一个follower迟迟未向leader回复备份成功时，出现阻塞的状态；</pre></td></tr><tr><td data-num="33"></td><td><pre>问题<span class="token number">2</span>：当leader回执给producer的ack丢失时，producer因为没有收到来自leader的ack，则默认数据没有发送成功，会重新向集群发送未收到ack的消息，导致数据的重复。 <span class="token comment">-- 数据的重复指：同一条消息重复发送。</span></pre></td></tr><tr><td data-num="34"></td><td><pre></pre></td></tr><tr><td data-num="35"></td><td><pre></pre></td></tr><tr><td data-num="36"></td><td><pre><span class="token comment">-- 那如何解决这两个问题呢？</span></pre></td></tr></table></figure><ul><li>问题 1（数据阻塞）解决方案：</li></ul><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre>规则：leader完成消息的读取和写出操作，follower定时向leader拉取数据。</pre></td></tr><tr><td data-num="2"></td><td><pre></pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token number">1.</span> leader维护了一个动态的<span class="token operator">in</span><span class="token operator">-</span>sync replicat <span class="token keyword">set</span> <span class="token punctuation">(</span>ISR<span class="token punctuation">)</span> 同步副本的列表，说明：即使是follower也有可能不在isr列表中。</pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token number">2.</span>。只要在isr列表中所有的follower均告知leader副本备份完成以后，则leader向producer回执ack，则不受限于出现故障的follower，因为出现故障，就被移除isr列表中。</pre></td></tr><tr><td data-num="5"></td><td><pre></pre></td></tr><tr><td data-num="6"></td><td><pre><span class="token comment">-- 问题 1：</span></pre></td></tr><tr><td data-num="7"></td><td><pre>那么什么情况下follower不在isr列表呢？</pre></td></tr><tr><td data-num="8"></td><td><pre><span class="token comment">-- 答案：</span></pre></td></tr><tr><td data-num="9"></td><td><pre>如果follower没有在规定的时间与leader保持同步，则leader会将该follower从isr中踢出，同步最大时间通过replica<span class="token punctuation">.</span>lag<span class="token punctuation">.</span><span class="token keyword">time</span><span class="token punctuation">.</span>max<span class="token punctuation">.</span>ms参数设定。</pre></td></tr><tr><td data-num="10"></td><td><pre></pre></td></tr><tr><td data-num="11"></td><td><pre><span class="token comment">-- 问题 2：</span></pre></td></tr><tr><td data-num="12"></td><td><pre>那么从isr中踢出的follower怎么重新回到isr中呢？ <span class="token comment">-- 故障处理机制</span></pre></td></tr><tr><td data-num="13"></td><td><pre><span class="token comment">-- 答案：</span></pre></td></tr><tr><td data-num="14"></td><td><pre>每个消息在follower的log文件中有：</pre></td></tr><tr><td data-num="15"></td><td><pre><span class="token number">1</span><span class="token punctuation">)</span> 真实数据 :消息的真实数据</pre></td></tr><tr><td data-num="16"></td><td><pre><span class="token number">2</span><span class="token punctuation">)</span> LEO<span class="token punctuation">(</span>log <span class="token keyword">end</span> <span class="token keyword">offset</span><span class="token punctuation">)</span> : 消息的最后偏移量</pre></td></tr><tr><td data-num="17"></td><td><pre><span class="token number">3</span><span class="token punctuation">)</span> HW<span class="token punctuation">(</span>High Watermark<span class="token punctuation">)</span> ：ISR列表中follower最小的LEO（偏移量）</pre></td></tr><tr><td data-num="18"></td><td><pre></pre></td></tr><tr><td data-num="19"></td><td><pre>说明：</pre></td></tr><tr><td data-num="20"></td><td><pre><span class="token number">1</span>）每个follower中的LEO可能是不一样的，因副本同步的快慢有差异；</pre></td></tr><tr><td data-num="21"></td><td><pre><span class="token number">2</span>）leader中log的LEO是最大的，因为数据源源不断的发送过来，它的落盘速度是最快的；</pre></td></tr><tr><td data-num="22"></td><td><pre><span class="token number">3</span>）HW之前的数据对consumer可见；</pre></td></tr><tr><td data-num="23"></td><td><pre><span class="token number">4</span>）HW是一个动态的数据，当leader回执ack一次HW就会更新一次。</pre></td></tr><tr><td data-num="24"></td><td><pre></pre></td></tr><tr><td data-num="25"></td><td><pre>follower发生故障后会被临时踢出ISR，待该follower恢复后，follower会读取本地磁盘记录的上次的HW，并将log文件高于HW的部分截取掉，从HW开始向leader进行同步。等该follower的LEO大于等于该<span class="token keyword">Partition</span>的HW，即follower追上leader之后，就可以重新加入ISR了。</pre></td></tr><tr><td data-num="26"></td><td><pre></pre></td></tr><tr><td data-num="27"></td><td><pre><span class="token comment">-- 问题 3：</span></pre></td></tr><tr><td data-num="28"></td><td><pre>当leader挂掉以后怎么办？</pre></td></tr><tr><td data-num="29"></td><td><pre><span class="token comment">-- 答案：</span></pre></td></tr><tr><td data-num="30"></td><td><pre><span class="token number">1</span>） 重新选举leader；</pre></td></tr><tr><td data-num="31"></td><td><pre><span class="token number">2</span>） 从isr列表中的follower中选取；</pre></td></tr><tr><td data-num="32"></td><td><pre><span class="token number">3</span>） 随机选择。</pre></td></tr><tr><td data-num="33"></td><td><pre><span class="token string">'详细过程'</span>：leader发生故障之后，会从ISR中选出一个新的leader，之后，为保证多个副本之间的数据一致性，其余的follower会先将各自的log文件高于HW的部分截掉，然后从新的leader同步数据。</pre></td></tr><tr><td data-num="34"></td><td><pre><span class="token string">'注意'</span>：这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复</pre></td></tr></table></figure><ul><li>问题 2（数据重复）的解决方案</li></ul><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token comment">-- 在 0.11 之前的 kafka 版本：</span></pre></td></tr><tr><td data-num="2"></td><td><pre>在消费者端进行去重，在producer传输数据时，对消息增加唯一的全局主键，然后在消费端根据该主键进行去重。 </pre></td></tr><tr><td data-num="3"></td><td><pre>该方式导致消费者组所有的消费者都需要进行去重操作，重复。</pre></td></tr><tr><td data-num="4"></td><td><pre></pre></td></tr><tr><td data-num="5"></td><td><pre><span class="token comment">-- 在 0.11 版本之后引进了 Exactly Once （幂等性）来解决数据重复的问题</span></pre></td></tr><tr><td data-num="6"></td><td><pre> <span class="token number">1</span>） Exactly Once （幂等性） ： 做n次和做一次的效果是一样的，就是指Producer不论向Server发送多少次重复数据（重复发送同一条数据），Server端都只会持久化一条，在server端完成去重操作。</pre></td></tr><tr><td data-num="7"></td><td><pre> <span class="token number">2</span>） 幂等性实现过程</pre></td></tr><tr><td data-num="8"></td><td><pre>   初始化数据时，给消息分配一个pid，发往同一个分区的消息会附带sequence Number<span class="token punctuation">,</span>broker端会将<span class="token operator">&lt;</span>pid<span class="token punctuation">,</span><span class="token keyword">partition</span><span class="token punctuation">,</span>sequence Number<span class="token operator">></span>和消息的真实数据一起存储到log文件中，当具有相同主键的消息提交时，Broker只会持久化一条。</pre></td></tr><tr><td data-num="9"></td><td><pre></pre></td></tr><tr><td data-num="10"></td><td><pre><span class="token comment">-- 何为主键？</span></pre></td></tr><tr><td data-num="11"></td><td><pre>由<span class="token operator">&lt;</span>pid<span class="token punctuation">,</span><span class="token keyword">partition</span><span class="token punctuation">,</span>sequence Number<span class="token operator">></span>三个参数构成的集合。重复发送的数据，这三个值不会变，数据是否重复与数据的内容无关，而是指为同一条数据多次发送。 <span class="token comment">-- 总结：重发的消息的主键是不会改变的，新发的消息 seqnumber 就会变化。</span></pre></td></tr><tr><td data-num="12"></td><td><pre></pre></td></tr><tr><td data-num="13"></td><td><pre>例如：消息A与消息B的数据内容完全一致</pre></td></tr><tr><td data-num="14"></td><td><pre>producer向集群发送消息A，集群收到以后返回的ack丢失，则消息A会被再次发送一次，此时消息A的主键是和第一次发送时相同，则集群认为数据是重复，不会进行存储；</pre></td></tr><tr><td data-num="15"></td><td><pre>producer向集群发送消息B，虽然与消息A的数据相同，但是seqnumber是不同的，所以不是重复的数据，集群会进行数据存储。</pre></td></tr><tr><td data-num="16"></td><td><pre></pre></td></tr><tr><td data-num="17"></td><td><pre>说明：</pre></td></tr><tr><td data-num="18"></td><td><pre><span class="token number">1</span>） sequence Number ：消息序列号，发往同一<span class="token keyword">Partition</span>的消息会附带Sequence Number，表示该producer向该分区发送的第几次消息；</pre></td></tr><tr><td data-num="19"></td><td><pre><span class="token number">2</span><span class="token punctuation">)</span>	pid : 生产者的id； </pre></td></tr><tr><td data-num="20"></td><td><pre><span class="token number">3</span><span class="token punctuation">)</span>  <span class="token keyword">partition</span> ： 分区号；</pre></td></tr><tr><td data-num="21"></td><td><pre><span class="token number">4</span>） PID重启就会变化，同时不同的<span class="token keyword">Partition</span>也具有不同主键，所以幂等性无法保证跨分区跨会话的Exactly Once；</pre></td></tr><tr><td data-num="22"></td><td><pre><span class="token number">5</span>） 开启幂等性会降低kafka的性能；</pre></td></tr><tr><td data-num="23"></td><td><pre><span class="token number">6</span>） 幂等性的底层原理也还是通过给消息增加全局的唯一主键的方式；</pre></td></tr><tr><td data-num="24"></td><td><pre><span class="token number">7</span>） 开启幂等性参数：<span class="token keyword">enable</span><span class="token punctuation">.</span>idompotence设置为<span class="token boolean">true</span>即可。</pre></td></tr></table></figure><h3 id="33-kafka之消费者-consumer"><a class="anchor" href="#33-kafka之消费者-consumer">#</a> 3.3 Kafka 之消费者 consumer</h3><h4 id="331-消费模式"><a class="anchor" href="#331-消费模式">#</a> 3.3.1 消费模式</h4><p>​	消费者从 server 中读取数据的方式有两种：pull （拉）和 push（推）</p><ol><li><p>pull ： consumer 向 server 拉取数据 <strong>【主动】</strong></p><ul><li>优点：消费者按需索取</li><li>缺点：不及时，有可能拉取到了空数据</li></ul></li><li><p>push ：server 向 consumer 推送数据 **【被动】**</p><ul><li>优点：及时</li><li>缺点：推送的速率与消费者消费的数据不一致时，产生背压</li></ul></li><li><p>kafka 默认使用 pull，拉取数据的方式。因为 kafka 是一对多的关系，同一个消费者组内的不同消费者的消费速率不同，所以不好设定推送的速率。</p></li><li><p>当出现拉取的数据为空时，consumer 会等待一段时间之后再拉取数据，这段时长即为 timeout</p></li></ol><h4 id="332-分区分配策略"><a class="anchor" href="#332-分区分配策略">#</a> 3.3.2 分区分配策略</h4><p>​	三种方式：roundrobin 、 range 、sticky</p><ol><li><p>roundrobin ： 轮询的方式 ，理解为洗牌，一张一张的发，分区一个一个轮询的方式分配给消费者；</p><p>缺点：当有新的消费者加进来时，所有的分区需要重新分配分区，基本上大多数的消费者的消费分区都会发生改变。</p></li><li><p>range：理解斗地主把牌按数量平均分配；</p><p>缺点：订阅的话题过多时，存在分区数量不均等的情况。</p></li><li><p>sticky：是在第一种方式的基础上进行改进，解决新增消费者情况的缺点，此时不再是所有消费者的分区进行重新分配，而是新进的消费者取之前所有消费者最后一次分区的数据进行消费。</p></li></ol><ul><li>当消费者的个数 &gt; 分区的个数时，有些消费者没分配不到数据。</li><li>消费者默认的分区分配策略是 range，但是消费者在消费数据时也可以自定指定策略。</li><li>一个分区只能由一个消费者进行消费。</li></ul><h4 id="333-offset的维护"><a class="anchor" href="#333-offset的维护">#</a> 3.3.3 offset 的维护</h4><p>​ 由于 consumer 在消费过程中可能会出现断电宕机等故障，consumer 恢复后，需要从故障前的位置的继续消费，所以 consumer 需要实时记录自己消费到了哪个 offset，以便故障恢复后继续消费。<br>​ Kafka 0.9 版本之前，consumer 默认将 offset 保存在 Zookeeper 中，从 0.9 版本开始，consumer 默认将 offset 保存在 Kafka 一个内置的 topic 中，该 topic 为__consumer_offsets。</p><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token comment">-- 问题：</span></pre></td></tr><tr><td data-num="2"></td><td><pre>为什么要将<span class="token keyword">offset</span>从zookeeper中转移到kafka中？</pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token comment">-- 回答：</span></pre></td></tr><tr><td data-num="4"></td><td><pre>zookeeper中维护<span class="token keyword">offset</span>的效率对于Kafka来说，不可控的，Kafka不能通过修改自己的代码来提升zookeeper维护<span class="token keyword">offset</span>的效率，所以将<span class="token keyword">offset</span>的维护迁移到kafka的会话中。</pre></td></tr></table></figure><h3 id="34-kafka高效读写数据"><a class="anchor" href="#34-kafka高效读写数据">#</a> 3.4 Kafka 高效读写数据</h3><h4 id="341-顺序写磁盘"><a class="anchor" href="#341-顺序写磁盘">#</a> 3.4.1 顺序写磁盘</h4><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token comment">-- 问题 1：</span></pre></td></tr><tr><td data-num="2"></td><td><pre>kafka的producer生产的数据最终按照顺序存储到磁盘上，写入到磁盘中数据过程不是很慢吗？</pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token comment">-- 回答：</span></pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token number">1</span>） 多分区存储模式：kafka是采用多分区的存储方式，提高了高并发；</pre></td></tr><tr><td data-num="5"></td><td><pre><span class="token number">2</span>） 顺序写模式：按照顺序写的速度能够减少大量磁头询地址的时间，使写数据速度和网络传输速度相当，所以基本上够用，但是还是比内存数据传输的速度要慢。</pre></td></tr></table></figure><h4 id="342-应用pagecache"><a class="anchor" href="#342-应用pagecache">#</a> 3.4.2 应用 Pagecache</h4><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token comment">-- 1. 说明：</span></pre></td></tr><tr><td data-num="2"></td><td><pre>Pagecache<span class="token punctuation">(</span>网页缓存<span class="token punctuation">)</span>：是操作系统实现的一个功能，因为linux系统兼容这个功能，所以kafka能够使用，解决大量随机读写的过程。</pre></td></tr><tr><td data-num="3"></td><td><pre></pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token comment">-- 2. 内存：</span></pre></td></tr><tr><td data-num="5"></td><td><pre>我们常说的内存可以分为两个模块，一是提供给系统的内核使用，此部分对于用户是不可见的，不能被用户使用，二是供用户使用的内存。</pre></td></tr><tr><td data-num="6"></td><td><pre></pre></td></tr><tr><td data-num="7"></td><td><pre><span class="token comment">-- 3. 原理： </span></pre></td></tr><tr><td data-num="8"></td><td><pre>pagecache是在内核内存中开辟的一个内存空间，producer生产的数据，先会存储在该内存中，待达到一定的数据量以后，再统一进行落盘，当消费者消费的速率和生产者生产的速率相同时，读写的效率是最高的，因为此时生产的数据不需要落盘处理，consumer直接从内存中读取数据。</pre></td></tr><tr><td data-num="9"></td><td><pre></pre></td></tr><tr><td data-num="10"></td><td><pre><span class="token comment">-- 4. 交换区和 pagecache 的区别：</span></pre></td></tr><tr><td data-num="11"></td><td><pre>交换区：将磁盘当做内存使用；</pre></td></tr><tr><td data-num="12"></td><td><pre>pagecache：将内存当做磁盘使用；</pre></td></tr><tr><td data-num="13"></td><td><pre>恰好是两个相反的过程。</pre></td></tr><tr><td data-num="14"></td><td><pre></pre></td></tr><tr><td data-num="15"></td><td><pre><span class="token comment">-- 5. 假如 pagecache 挂掉了怎么办？内存中的数据不是丢失了吗？</span></pre></td></tr><tr><td data-num="16"></td><td><pre>首先当发生这个问题时，是不能够完全保证数据一定不丢失，但是由于kafka具有副本策略，所以有一定保证的。</pre></td></tr></table></figure><p>优点：</p><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token number">1</span>） I<span class="token operator">/</span>O Scheduler 会将连续的小块写组装成大块的物理写从而提高性能</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token number">2</span>） I<span class="token operator">/</span>O Scheduler 会尝试将一些写操作重新按顺序排好，从而减少磁盘头的移动时间</pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token number">3</span>） 充分利用所有空闲内存（非 JVM 内存）。如果使用应用层 Cache（即 JVM 堆内存），会增加 GC 负担</pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token number">4</span>） 读操作可直接在 Page Cache 内进行。如果消费和生产速度相当，甚至不需要通过物理磁盘（直接通过 Page Cache）交换数据</pre></td></tr><tr><td data-num="5"></td><td><pre><span class="token number">5</span>） 如果进程重启，JVM 内的 Cache 会失效，但 Page Cache 仍然可用</pre></td></tr></table></figure><h4 id="343-零拷贝技术"><a class="anchor" href="#343-零拷贝技术">#</a> 3.4.3 零拷贝技术</h4><p><img data-src="https://miyazono-1255488789.cos.ap-shanghai.myqcloud.com/markdown/20210822155816.png" alt="图5"></p><p>说明：</p><p>内存是分级别的，读写数据时数据先经过内核内存再经过用户内存。</p><p>如果是数据的写出操作，则数据经过内核内存以后就直接往外写出，不需经过用户内存，用户内存只是负责调度的功能，减少了 数据的传输过程，这个过程称为零拷贝。</p><h3 id="35-zookeeper在kafka中的作用"><a class="anchor" href="#35-zookeeper在kafka中的作用">#</a> 3.5 zookeeper 在 kafka 中的作用</h3><ul><li>kafka 是一个去中心化的框架，没有主从之分，则需要一个中央控制中心进行调度，类似 ha 集群一样。</li><li>kafka 是依赖于 zookeeper 集群的。</li></ul><p>流程：一个 kafka 集群，多个 broker，一个 zk 集群</p><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token comment">-- 步骤：</span></pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token number">1</span>） 首先所有的broker会竞选一个controller（随机竞选，谁厉害谁上），负责管理集群broker的上下线，所有topic的分区副本分配和leader选举等工作；</pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token number">2</span>） 所有的broker将自己的id信息注册到zk集群的节点上；</pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token number">3</span>） controller监控zk的这个信息；</pre></td></tr><tr><td data-num="5"></td><td><pre><span class="token number">4</span>） controller负责broker的leader选举工作；</pre></td></tr><tr><td data-num="6"></td><td><pre><span class="token number">5</span>） broker将状态信息注册到zk集群上；</pre></td></tr><tr><td data-num="7"></td><td><pre><span class="token number">6</span>） 此时分区的leader故障以后，controller从zk集群中获取isr中的follower信息，负责从isr中follower选举出一个新的leader；</pre></td></tr><tr><td data-num="8"></td><td><pre><span class="token number">7</span>） controller更新zk集群上broker的状态信息。</pre></td></tr><tr><td data-num="9"></td><td><pre></pre></td></tr><tr><td data-num="10"></td><td><pre><span class="token comment">-- 假如故障的 leader 恰好也是 controller 怎么办？</span></pre></td></tr><tr><td data-num="11"></td><td><pre>先从现存的follower中重新选举controller，再执行<span class="token number">1</span><span class="token operator">-</span><span class="token number">5</span>步。</pre></td></tr></table></figure><p><img data-src="https://miyazono-1255488789.cos.ap-shanghai.myqcloud.com/markdown/20210822155820.png" alt="图6"></p><h3 id="36-kafka事务"><a class="anchor" href="#36-kafka事务">#</a> 3.6 Kafka 事务</h3><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token comment">-- 问题：</span></pre></td></tr><tr><td data-num="2"></td><td><pre>事务用来解决什么问题？</pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token comment">-- 回答：</span></pre></td></tr><tr><td data-num="4"></td><td><pre>kafka使用Exactly Once解决producer端生产数据重复的问题存在什么 问题？</pre></td></tr><tr><td data-num="5"></td><td><pre></pre></td></tr><tr><td data-num="6"></td><td><pre>问题<span class="token number">1</span>：不能跨分区；</pre></td></tr><tr><td data-num="7"></td><td><pre>问题<span class="token number">2</span>：producer重启时，pid会发生变化。</pre></td></tr><tr><td data-num="8"></td><td><pre></pre></td></tr><tr><td data-num="9"></td><td><pre>则事务就是来解决上面问题的，事务可以保证Kafka在Exactly Once语义的基础上，生产和消费可以跨分区和会话，要么全部成功，要么全部失败。</pre></td></tr><tr><td data-num="10"></td><td><pre></pre></td></tr><tr><td data-num="11"></td><td><pre><span class="token comment">-- 那具体是怎么做的呢？</span></pre></td></tr></table></figure><h4 id="361-producer事务"><a class="anchor" href="#361-producer事务">#</a> 3.6.1 producer 事务</h4><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token comment">-- 解决 producer 重启问题：</span></pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token number">1</span><span class="token punctuation">)</span> 引进全局唯一的<span class="token keyword">Transaction</span> ID，将producer的pid与<span class="token keyword">Transaction</span> ID进行绑定。当重启producer时，可以通过正在进行的<span class="token keyword">Transaction</span> ID获得原来的PID<span class="token punctuation">.</span></pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token number">2</span><span class="token punctuation">)</span> 为了管理<span class="token keyword">Transaction</span>，Kafka引入了一个新的组件<span class="token keyword">Transaction</span> Coordinator。Producer就是通过和<span class="token keyword">Transaction</span> Coordinator交互获得<span class="token keyword">Transaction</span> ID对应的任务状态。<span class="token keyword">Transaction</span> Coordinator还负责将所有事务写入Kafka的一个内部Topic，这样即使整个服务重启，由于事务状态得到保存，进行中的事务状态可以得到恢复，从而继续进行。</pre></td></tr></table></figure><h4 id="362-consumer事务精准一次性消费"><a class="anchor" href="#362-consumer事务精准一次性消费">#</a> 3.6.2 Consumer 事务（精准一次性消费）</h4><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre>kafak对consumer事务的保证是非常弱的，尤其无法保证<span class="token keyword">Commit</span>的信息被精确消费。这是由于Consumer可以通过<span class="token keyword">offset</span>访问任意信息，而且不同的Segment <span class="token keyword">File</span>生命周期不同，同一事务的消息可能会出现重启后被删除的情况</pre></td></tr></table></figure><h2 id="四-kafka-api"><a class="anchor" href="#四-kafka-api">#</a> 四、 Kafka API</h2><p>温馨提示</p><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre>api的步骤：</pre></td></tr><tr><td data-num="2"></td><td><pre></pre></td></tr><tr><td data-num="3"></td><td><pre>第一步： new 对象<span class="token punctuation">;</span></pre></td></tr><tr><td data-num="4"></td><td><pre>第二步： 具体的操作<span class="token punctuation">;</span></pre></td></tr><tr><td data-num="5"></td><td><pre>第三步： 关闭资源。</pre></td></tr><tr><td data-num="6"></td><td><pre></pre></td></tr><tr><td data-num="7"></td><td><pre><span class="token comment">-- 不知道要写哪些参数？不知道参数的意义？不知道参数取值？怎么办？</span></pre></td></tr><tr><td data-num="8"></td><td><pre>请认准kafka官网：https:<span class="token comment">//kafka.apache.org/documentation/</span></pre></td></tr><tr><td data-num="9"></td><td><pre>producer API ： 找Producer Configs</pre></td></tr><tr><td data-num="10"></td><td><pre>consumer API：  找Consumer Configs</pre></td></tr></table></figure><h3 id="41-producer-api"><a class="anchor" href="#41-producer-api">#</a> 4.1 Producer API</h3><h4 id="411-消息发送流程"><a class="anchor" href="#411-消息发送流程">#</a> 4.1.1 消息发送流程</h4><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token comment">-- 问题： </span></pre></td></tr><tr><td data-num="2"></td><td><pre>Kafka的Producer发送消息采用的是异步发送的方式，这种方式优点和缺点是什么呢？</pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token comment">-- 回答：</span></pre></td></tr><tr><td data-num="4"></td><td><pre>优点：高效率，生产者只要一直生产数据就可以，不需要等到ack回执后再进行生产数据；</pre></td></tr><tr><td data-num="5"></td><td><pre>缺点：不能实时知道数据是否发送成功，不过有ack机制、幂等性机制和producer事务（保证数据的准确性）。</pre></td></tr></table></figure><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token comment">-- 发送数据的流程：</span></pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token string">'两线程一共享变量'</span>：</pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token number">1.</span> main线程：将消息发送给RecordAccumulator</pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token number">2.</span> Sender线程：Sender线程不断从RecordAccumulator中拉取消息发送到Kafka broker</pre></td></tr><tr><td data-num="5"></td><td><pre><span class="token number">3.</span> 线程共享变量——RecordAccumulator：数据临时存储器。</pre></td></tr><tr><td data-num="6"></td><td><pre><span class="token string">'步骤'</span></pre></td></tr><tr><td data-num="7"></td><td><pre>第一步：生产者首先将数据包装成ProducerRecord</pre></td></tr><tr><td data-num="8"></td><td><pre>第二步：main线程中有一个send方法，producer将ProducerRecord发送给interceptors<span class="token string">'拦截器'</span>处理；</pre></td></tr><tr><td data-num="9"></td><td><pre>第三步：interceptors处理好后将数据传递给<span class="token string">'序列化器'</span>，将数据序列化； <span class="token comment">-- 在 producer 端序列化</span></pre></td></tr><tr><td data-num="10"></td><td><pre>第四步：将序列化好的数据传递给<span class="token string">'分区器'</span>，对数据进行分区； <span class="token comment">-- 在 producer 端序列化</span></pre></td></tr><tr><td data-num="11"></td><td><pre>第五步：将数据传递到内存的数据缓存区，在这里面，话题有多少个分区，在缓存区里面就有多少个分区，一一对应，对应的分区数据就会去到对应的缓存区的分区中； <span class="token comment">-- 此时的数据是已经分好区了，同时也是已经序列化，此时 producer 就不再管这里的数据了；</span></pre></td></tr><tr><td data-num="12"></td><td><pre>第六步：Sender线程就将数据发送给topic中的分区中。 </pre></td></tr><tr><td data-num="13"></td><td><pre></pre></td></tr><tr><td data-num="14"></td><td><pre><span class="token comment">-- 此时的数据，Sender 线程是怎么向 topic 中发的呢？</span></pre></td></tr><tr><td data-num="15"></td><td><pre>batch<span class="token punctuation">.</span>size：只有数据积累到batch<span class="token punctuation">.</span>size之后，sender才会发送数据。</pre></td></tr><tr><td data-num="16"></td><td><pre>linger<span class="token punctuation">.</span>ms：如果数据迟迟未达到batch<span class="token punctuation">.</span>size，sender等待linger<span class="token punctuation">.</span><span class="token keyword">time</span>之后就会发送数据。</pre></td></tr></table></figure><p><img data-src="https://miyazono-1255488789.cos.ap-shanghai.myqcloud.com/markdown/20210822155824.png" alt="图7"></p><h4 id="412-异步发送api"><a class="anchor" href="#412-异步发送api">#</a> 4.1.2 异步发送 API</h4><figure class="highlight java"><figcaption data-lang="java"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">package</span> <span class="token namespace">kafkaproducer</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="2"></td><td><pre></pre></td></tr><tr><td data-num="3"></td><td><pre></pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>clients<span class="token punctuation">.</span>producer<span class="token punctuation">.</span></span><span class="token class-name">Callback</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="5"></td><td><pre><span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>clients<span class="token punctuation">.</span>producer<span class="token punctuation">.</span></span><span class="token class-name">KafkaProducer</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="6"></td><td><pre><span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>clients<span class="token punctuation">.</span>producer<span class="token punctuation">.</span></span><span class="token class-name">ProducerRecord</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="7"></td><td><pre><span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>clients<span class="token punctuation">.</span>producer<span class="token punctuation">.</span></span><span class="token class-name">RecordMetadata</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="8"></td><td><pre></pre></td></tr><tr><td data-num="9"></td><td><pre><span class="token keyword">import</span> <span class="token namespace">java<span class="token punctuation">.</span>util<span class="token punctuation">.</span></span><span class="token class-name">Properties</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="10"></td><td><pre><span class="token keyword">import</span> <span class="token namespace">java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>concurrent<span class="token punctuation">.</span></span><span class="token class-name">ExecutionException</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="11"></td><td><pre><span class="token keyword">import</span> <span class="token namespace">java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>concurrent<span class="token punctuation">.</span></span><span class="token class-name">Future</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="12"></td><td><pre></pre></td></tr><tr><td data-num="13"></td><td><pre><span class="token comment">/**</span></pre></td></tr><tr><td data-num="14"></td><td><pre> * @author lianzhipeng</pre></td></tr><tr><td data-num="15"></td><td><pre> * @Description</pre></td></tr><tr><td data-num="16"></td><td><pre> * @create 2020-05-08 14:58</pre></td></tr><tr><td data-num="17"></td><td><pre> */</pre></td></tr><tr><td data-num="18"></td><td><pre></pre></td></tr><tr><td data-num="19"></td><td><pre><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">Producer</span> <span class="token punctuation">&#123;</span></pre></td></tr><tr><td data-num="20"></td><td><pre>    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token class-name">String</span><span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token keyword">throws</span> <span class="token class-name">ExecutionException</span><span class="token punctuation">,</span> <span class="token class-name">InterruptedException</span> <span class="token punctuation">&#123;</span></pre></td></tr><tr><td data-num="21"></td><td><pre></pre></td></tr><tr><td data-num="22"></td><td><pre>        <span class="token comment">//1.new 对象</span></pre></td></tr><tr><td data-num="23"></td><td><pre></pre></td></tr><tr><td data-num="24"></td><td><pre>        <span class="token class-name">Properties</span> properties <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Properties</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="25"></td><td><pre>        properties<span class="token punctuation">.</span><span class="token function">setProperty</span><span class="token punctuation">(</span><span class="token string">"key.serializer"</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="26"></td><td><pre>                <span class="token string">"org.apache.kafka.common.serialization.StringSerializer"</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="27"></td><td><pre>        properties<span class="token punctuation">.</span><span class="token function">setProperty</span><span class="token punctuation">(</span><span class="token string">"value.serializer"</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="28"></td><td><pre>                <span class="token string">"org.apache.kafka.common.serialization.StringSerializer"</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="29"></td><td><pre>        properties<span class="token punctuation">.</span><span class="token function">setProperty</span><span class="token punctuation">(</span><span class="token string">"acks"</span><span class="token punctuation">,</span> <span class="token string">"all"</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="30"></td><td><pre>        properties<span class="token punctuation">.</span><span class="token function">setProperty</span><span class="token punctuation">(</span><span class="token string">"bootstrap.servers"</span><span class="token punctuation">,</span> <span class="token string">"hadoop102:9092"</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="31"></td><td><pre></pre></td></tr><tr><td data-num="32"></td><td><pre>        <span class="token class-name">KafkaProducer</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token class-name">String</span><span class="token punctuation">></span></span> producer <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">KafkaProducer</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token class-name">String</span><span class="token punctuation">></span></span><span class="token punctuation">(</span>properties<span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="33"></td><td><pre></pre></td></tr><tr><td data-num="34"></td><td><pre>        <span class="token comment">//2. 具体的操作</span></pre></td></tr><tr><td data-num="35"></td><td><pre>        <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> <span class="token number">100</span><span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span></pre></td></tr><tr><td data-num="36"></td><td><pre>            <span class="token class-name">Future</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">RecordMetadata</span><span class="token punctuation">></span></span> result <span class="token operator">=</span> producer<span class="token punctuation">.</span><span class="token function">send</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">ProducerRecord</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token class-name">String</span><span class="token punctuation">></span></span><span class="token punctuation">(</span></pre></td></tr><tr><td data-num="37"></td><td><pre>                    <span class="token string">"first"</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="38"></td><td><pre>                    <span class="token string">"Message"</span> <span class="token operator">+</span> i<span class="token punctuation">,</span></pre></td></tr><tr><td data-num="39"></td><td><pre>                    <span class="token string">"这是第"</span> <span class="token operator">+</span> i <span class="token operator">+</span> <span class="token string">"条信息"</span></pre></td></tr><tr><td data-num="40"></td><td><pre>            <span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">Callback</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span><span class="token comment">// 回调函数，当 producer 发送的数据完成以后，返回告诉 producer 数据发送成功</span></pre></td></tr><tr><td data-num="41"></td><td><pre>                <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">onCompletion</span><span class="token punctuation">(</span><span class="token class-name">RecordMetadata</span> metadata<span class="token punctuation">,</span> <span class="token class-name">Exception</span> exception<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span></pre></td></tr><tr><td data-num="42"></td><td><pre>                    <span class="token keyword">int</span> partition <span class="token operator">=</span> metadata<span class="token punctuation">.</span><span class="token function">partition</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="43"></td><td><pre>                    <span class="token class-name">String</span> topic <span class="token operator">=</span> metadata<span class="token punctuation">.</span><span class="token function">topic</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="44"></td><td><pre>                    <span class="token keyword">long</span> offset <span class="token operator">=</span> metadata<span class="token punctuation">.</span><span class="token function">offset</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="45"></td><td><pre>                    <span class="token class-name">System</span><span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span></pre></td></tr><tr><td data-num="46"></td><td><pre>                            topic <span class="token operator">+</span> <span class="token string">"话题"</span></pre></td></tr><tr><td data-num="47"></td><td><pre>                                    <span class="token operator">+</span> partition <span class="token operator">+</span> <span class="token string">"分区"</span></pre></td></tr><tr><td data-num="48"></td><td><pre>                                    <span class="token operator">+</span> offset <span class="token operator">+</span> <span class="token string">"消息发送成功"</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="49"></td><td><pre>                <span class="token punctuation">&#125;</span></pre></td></tr><tr><td data-num="50"></td><td><pre>            <span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="51"></td><td><pre>            <span class="token comment">/*</span></pre></td></tr><tr><td data-num="52"></td><td><pre>           </pre></td></tr><tr><td data-num="53"></td><td><pre>            如下一行代码产生同步回调和异同回调两种方式：</pre></td></tr><tr><td data-num="54"></td><td><pre>            同步回调：加了此行代码，生产者收到 ack 以后再发第二条消息；类似打电话</pre></td></tr><tr><td data-num="55"></td><td><pre>            异步回调：未加此行代码，生成者只要一直发送消息既可。类似发短信</pre></td></tr><tr><td data-num="56"></td><td><pre>          </pre></td></tr><tr><td data-num="57"></td><td><pre>            */</pre></td></tr><tr><td data-num="58"></td><td><pre>            <span class="token class-name">RecordMetadata</span> recordMetadata <span class="token operator">=</span> result<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="59"></td><td><pre></pre></td></tr><tr><td data-num="60"></td><td><pre>            <span class="token class-name">System</span><span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"第"</span> <span class="token operator">+</span> i <span class="token operator">+</span> <span class="token string">"条消息发送结束"</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="61"></td><td><pre></pre></td></tr><tr><td data-num="62"></td><td><pre>        <span class="token punctuation">&#125;</span></pre></td></tr><tr><td data-num="63"></td><td><pre></pre></td></tr><tr><td data-num="64"></td><td><pre>        <span class="token comment">//3. 关闭资源，资源关闭的时候会调用回调函数</span></pre></td></tr><tr><td data-num="65"></td><td><pre>        producer<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="66"></td><td><pre></pre></td></tr><tr><td data-num="67"></td><td><pre>    <span class="token punctuation">&#125;</span></pre></td></tr><tr><td data-num="68"></td><td><pre><span class="token punctuation">&#125;</span></pre></td></tr></table></figure><h3 id="42-consumer-api"><a class="anchor" href="#42-consumer-api">#</a> 4.2 Consumer API</h3><h4 id="421-数据漏消费和重复消费"><a class="anchor" href="#421-数据漏消费和重复消费">#</a> 4.2.1 数据漏消费和重复消费</h4><ol><li>消费者不用担心数据的可靠性问题，因为消费者消费以后的数据是不会从 kafka 集群中删除的。但是消费者要关心两个问题：</li></ol><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token comment">-- 问题 1 数据漏消费</span></pre></td></tr><tr><td data-num="2"></td><td><pre>什么时候会出现数据漏消费呢？</pre></td></tr><tr><td data-num="3"></td><td><pre>先提交<span class="token keyword">offset</span>后消费。</pre></td></tr><tr><td data-num="4"></td><td><pre>例如：消费者从kafka集群中获取了数据，此数据在消费的过程中出现故障延迟最后宕机，在故障期间<span class="token keyword">offset</span>已经提交至kafka集群，此时实际上数据并没有被使用，但是kafka集群上该消费者消费的数据偏移量已经更新了，重启消费者时，上一条数据不能被消费了，导致数据漏消费。</pre></td></tr><tr><td data-num="5"></td><td><pre><span class="token comment">-- 问题 2 数据重复消费</span></pre></td></tr><tr><td data-num="6"></td><td><pre>什么时候会出现数据库重复消费呢？</pre></td></tr><tr><td data-num="7"></td><td><pre>当数据已经被消费以后，此时返回的<span class="token keyword">offset</span>时消费者出现了故障，则kafka集群中的_consumer_offset会话保存的<span class="token keyword">offset</span>则为上一次的数据，<span class="token keyword">offset</span>没有被更新，当消费者重新启动时，上一条数据则会被重新再消费一次。</pre></td></tr></table></figure><ol start="2"><li>谈谈消费者提交 offset 的模式</li></ol><p>消费者每次拉取数据的最大值为：1M，（ 1048576 字节）</p><ul><li><p>模式一：自动提交，默认每 5s 提交一次；</p></li><li><p>模式二：手动提交，两种方式：commitSync（同步提交）、commitAsync（异步提交）；</p></li></ul><p>​ 同步和异步的异同点：</p><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token comment">-- 相同点：</span></pre></td></tr><tr><td data-num="2"></td><td><pre>提交本次poll的一批数据最高的偏移量<span class="token punctuation">.</span></pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token comment">-- 不同点：</span></pre></td></tr><tr><td data-num="4"></td><td><pre>commitSync（同步提交）：提交<span class="token keyword">offset</span>时，commitSync阻塞当前线程，一直到提交成功，并且会自动失败重试（由不可控因素导致，也会出现提交失败）；</pre></td></tr><tr><td data-num="5"></td><td><pre>commitAsync（异步提交）：则没有失败重试机制，故有可能提交失败。</pre></td></tr></table></figure><h4 id="422-几个重要的参数"><a class="anchor" href="#422-几个重要的参数">#</a> 4.2.2 几个重要的参数</h4><ol><li>自动提交 offset 的时间：默认为 5s</li></ol><p><span class="exturl" data-url="aHR0cHM6Ly9rYWZrYS5hcGFjaGUub3JnL2RvY3VtZW50YXRpb24vI2F1dG8uY29tbWl0LmludGVydmFsLm1z">auto.commit.interval.ms</span></p><table><thead><tr><th>Type:</th><th>int</th></tr></thead><tbody><tr><td>Default:</td><td>5000</td></tr><tr><td>Valid Values:</td><td>[0,...]</td></tr><tr><td>Importance:</td><td>low</td></tr></tbody></table><ol start="2"><li>消费者消费数据的起始位置</li></ol><p><span class="exturl" data-url="aHR0cHM6Ly9rYWZrYS5hcGFjaGUub3JnL2RvY3VtZW50YXRpb24vI2F1dG8ub2Zmc2V0LnJlc2V0">auto.offset.reset</span></p><ul><li>earliest: automatically reset the offset to the earliest offset --&gt; 表示消费 topic 所有的数据</li><li>latest: automatically reset the offset to the latest offset --&gt; 表示只消费最新的数据</li></ul><table><thead><tr><th>Type:</th><th>string</th></tr></thead><tbody><tr><td>Default:</td><td>latest</td></tr><tr><td>Valid Values:</td><td>[latest, earliest, none]</td></tr><tr><td>Importance:</td><td>medium</td></tr></tbody></table><ol start="3"><li>一次从一个分区拉取的最大数据量</li></ol><p><span class="exturl" data-url="aHR0cHM6Ly9rYWZrYS5hcGFjaGUub3JnL2RvY3VtZW50YXRpb24vI21heC5wYXJ0aXRpb24uZmV0Y2guYnl0ZXM=">max.partition.fetch.bytes</span></p><table><thead><tr><th>Type:</th><th>int</th></tr></thead><tbody><tr><td>Default:</td><td>1048576</td></tr><tr><td>Valid Values:</td><td>[0,...]</td></tr><tr><td>Importance:</td><td>high</td></tr></tbody></table><h4 id="423-代码"><a class="anchor" href="#423-代码">#</a> 4.2.3 代码</h4><ul><li><h3 id="consumer-api"><a class="anchor" href="#consumer-api">#</a> Consumer API</h3></li></ul><figure class="highlight java"><figcaption data-lang="java"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">package</span> <span class="token namespace">kafkaconsumer</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="2"></td><td><pre></pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>clients<span class="token punctuation">.</span>consumer<span class="token punctuation">.</span></span><span class="token operator">*</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>common<span class="token punctuation">.</span></span><span class="token class-name">TopicPartition</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="5"></td><td><pre></pre></td></tr><tr><td data-num="6"></td><td><pre><span class="token keyword">import</span> <span class="token namespace">java<span class="token punctuation">.</span>time<span class="token punctuation">.</span></span><span class="token class-name">Duration</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="7"></td><td><pre><span class="token keyword">import</span> <span class="token namespace">java<span class="token punctuation">.</span>util<span class="token punctuation">.</span></span><span class="token class-name">Collections</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="8"></td><td><pre><span class="token keyword">import</span> <span class="token namespace">java<span class="token punctuation">.</span>util<span class="token punctuation">.</span></span><span class="token class-name">Map</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="9"></td><td><pre><span class="token keyword">import</span> <span class="token namespace">java<span class="token punctuation">.</span>util<span class="token punctuation">.</span></span><span class="token class-name">Properties</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="10"></td><td><pre></pre></td></tr><tr><td data-num="11"></td><td><pre><span class="token comment">/**</span></pre></td></tr><tr><td data-num="12"></td><td><pre> * @author lianzhipeng</pre></td></tr><tr><td data-num="13"></td><td><pre> * @Description</pre></td></tr><tr><td data-num="14"></td><td><pre> * @create 2020-05-08 21:04</pre></td></tr><tr><td data-num="15"></td><td><pre> */</pre></td></tr><tr><td data-num="16"></td><td><pre></pre></td></tr><tr><td data-num="17"></td><td><pre><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">MyConsumer</span> <span class="token punctuation">&#123;</span></pre></td></tr><tr><td data-num="18"></td><td><pre>    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token class-name">String</span><span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span></pre></td></tr><tr><td data-num="19"></td><td><pre></pre></td></tr><tr><td data-num="20"></td><td><pre></pre></td></tr><tr><td data-num="21"></td><td><pre>        <span class="token comment">//1 new 对象</span></pre></td></tr><tr><td data-num="22"></td><td><pre></pre></td></tr><tr><td data-num="23"></td><td><pre>        <span class="token class-name">Properties</span> properties <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Properties</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="24"></td><td><pre></pre></td></tr><tr><td data-num="25"></td><td><pre>        properties<span class="token punctuation">.</span><span class="token function">setProperty</span><span class="token punctuation">(</span><span class="token string">"key.deserializer"</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="26"></td><td><pre>                <span class="token string">"org.apache.kafka.common.serialization.StringDeserializer"</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="27"></td><td><pre>        properties<span class="token punctuation">.</span><span class="token function">setProperty</span><span class="token punctuation">(</span><span class="token string">"value.deserializer"</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="28"></td><td><pre>                <span class="token string">"org.apache.kafka.common.serialization.StringDeserializer"</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="29"></td><td><pre>        properties<span class="token punctuation">.</span><span class="token function">setProperty</span><span class="token punctuation">(</span><span class="token string">"bootstrap.servers"</span><span class="token punctuation">,</span> <span class="token string">"hadoop102:9092"</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="30"></td><td><pre>        properties<span class="token punctuation">.</span><span class="token function">setProperty</span><span class="token punctuation">(</span><span class="token string">"group.id"</span><span class="token punctuation">,</span> <span class="token string">"group9"</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="31"></td><td><pre>        properties<span class="token punctuation">.</span><span class="token function">setProperty</span><span class="token punctuation">(</span><span class="token string">"auto.offset.reset"</span><span class="token punctuation">,</span> <span class="token string">"earliest"</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="32"></td><td><pre>        <span class="token comment">// 自动提交 offset</span></pre></td></tr><tr><td data-num="33"></td><td><pre>        properties<span class="token punctuation">.</span><span class="token function">setProperty</span><span class="token punctuation">(</span><span class="token string">"enable.auto.commit"</span><span class="token punctuation">,</span><span class="token string">"false"</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="34"></td><td><pre></pre></td></tr><tr><td data-num="35"></td><td><pre></pre></td></tr><tr><td data-num="36"></td><td><pre>        <span class="token class-name">KafkaConsumer</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token class-name">String</span><span class="token punctuation">></span></span> consumer <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">KafkaConsumer</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token class-name">String</span><span class="token punctuation">></span></span><span class="token punctuation">(</span>properties<span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="37"></td><td><pre></pre></td></tr><tr><td data-num="38"></td><td><pre></pre></td></tr><tr><td data-num="39"></td><td><pre>        <span class="token comment">//2 操作</span></pre></td></tr><tr><td data-num="40"></td><td><pre>        <span class="token comment">// 连接话题</span></pre></td></tr><tr><td data-num="41"></td><td><pre>        consumer<span class="token punctuation">.</span><span class="token function">subscribe</span><span class="token punctuation">(</span><span class="token class-name">Collections</span><span class="token punctuation">.</span><span class="token function">singleton</span><span class="token punctuation">(</span><span class="token string">"first"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="42"></td><td><pre>        <span class="token comment">// 拉取数据</span></pre></td></tr><tr><td data-num="43"></td><td><pre>        <span class="token class-name">Duration</span> duration <span class="token operator">=</span> <span class="token class-name">Duration</span><span class="token punctuation">.</span><span class="token function">ofMillis</span><span class="token punctuation">(</span><span class="token number">500</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="44"></td><td><pre>        <span class="token keyword">while</span> <span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">)</span><span class="token punctuation">&#123;</span></pre></td></tr><tr><td data-num="45"></td><td><pre>            <span class="token class-name">ConsumerRecords</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token class-name">String</span><span class="token punctuation">></span></span> records <span class="token operator">=</span> consumer<span class="token punctuation">.</span><span class="token function">poll</span><span class="token punctuation">(</span>duration<span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="46"></td><td><pre>            <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token class-name">ConsumerRecord</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token class-name">String</span><span class="token punctuation">></span></span> <span class="token keyword">record</span> <span class="token operator">:</span> records<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span></pre></td></tr><tr><td data-num="47"></td><td><pre>                <span class="token class-name">System</span><span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token keyword">record</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="48"></td><td><pre>            <span class="token punctuation">&#125;</span></pre></td></tr><tr><td data-num="49"></td><td><pre>            <span class="token comment">// 手动同步提交</span></pre></td></tr><tr><td data-num="50"></td><td><pre><span class="token comment">//            consumer.commitSync();</span></pre></td></tr><tr><td data-num="51"></td><td><pre>            <span class="token comment">// 手动异步提交</span></pre></td></tr><tr><td data-num="52"></td><td><pre>            consumer<span class="token punctuation">.</span><span class="token function">commitAsync</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">OffsetCommitCallback</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span></pre></td></tr><tr><td data-num="53"></td><td><pre>                <span class="token annotation punctuation">@Override</span></pre></td></tr><tr><td data-num="54"></td><td><pre>                <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">onComplete</span><span class="token punctuation">(</span><span class="token class-name">Map</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">TopicPartition</span><span class="token punctuation">,</span> <span class="token class-name">OffsetAndMetadata</span><span class="token punctuation">></span></span> offsets<span class="token punctuation">,</span> <span class="token class-name">Exception</span> exception<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span></pre></td></tr><tr><td data-num="55"></td><td><pre>                    offsets<span class="token punctuation">.</span><span class="token function">forEach</span><span class="token punctuation">(</span></pre></td></tr><tr><td data-num="56"></td><td><pre>                            <span class="token punctuation">(</span>t<span class="token punctuation">,</span> o<span class="token punctuation">)</span> <span class="token operator">-></span> <span class="token punctuation">&#123;</span></pre></td></tr><tr><td data-num="57"></td><td><pre>                                <span class="token class-name">System</span><span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"分区："</span> <span class="token operator">+</span> t <span class="token operator">+</span> <span class="token string">"\nOffset："</span> <span class="token operator">+</span> o<span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="58"></td><td><pre>                            <span class="token punctuation">&#125;</span></pre></td></tr><tr><td data-num="59"></td><td><pre>                    <span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="60"></td><td><pre>                <span class="token punctuation">&#125;</span></pre></td></tr><tr><td data-num="61"></td><td><pre>            <span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="62"></td><td><pre>        <span class="token punctuation">&#125;</span></pre></td></tr><tr><td data-num="63"></td><td><pre>        <span class="token comment">//3 关闭资源</span></pre></td></tr><tr><td data-num="64"></td><td><pre><span class="token comment">//        consumer.close();</span></pre></td></tr><tr><td data-num="65"></td><td><pre>    <span class="token punctuation">&#125;</span></pre></td></tr><tr><td data-num="66"></td><td><pre></pre></td></tr><tr><td data-num="67"></td><td><pre><span class="token punctuation">&#125;</span></pre></td></tr></table></figure><ul><li>异步提交代码：</li></ul><figure class="highlight java"><figcaption data-lang="java"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token comment">// 手动异步提交方式，形参里面为回调对象。</span></pre></td></tr><tr><td data-num="2"></td><td><pre>            consumer<span class="token punctuation">.</span><span class="token function">commitAsync</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">OffsetCommitCallback</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span></pre></td></tr><tr><td data-num="3"></td><td><pre>                <span class="token comment">/*</span></pre></td></tr><tr><td data-num="4"></td><td><pre>               回调方式，当消费成功以后调用此方法并进行打印</pre></td></tr><tr><td data-num="5"></td><td><pre>                */</pre></td></tr><tr><td data-num="6"></td><td><pre>                <span class="token annotation punctuation">@Override</span></pre></td></tr><tr><td data-num="7"></td><td><pre>                <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">onComplete</span><span class="token punctuation">(</span><span class="token class-name">Map</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">TopicPartition</span><span class="token punctuation">,</span> <span class="token class-name">OffsetAndMetadata</span><span class="token punctuation">></span></span> offsets<span class="token punctuation">,</span> <span class="token class-name">Exception</span> exception<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span></pre></td></tr><tr><td data-num="8"></td><td><pre>                    offsets<span class="token punctuation">.</span><span class="token function">forEach</span><span class="token punctuation">(</span></pre></td></tr><tr><td data-num="9"></td><td><pre>                            <span class="token punctuation">(</span>t<span class="token punctuation">,</span> o<span class="token punctuation">)</span> <span class="token operator">-></span> <span class="token punctuation">&#123;</span></pre></td></tr><tr><td data-num="10"></td><td><pre>                                <span class="token class-name">System</span><span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"分区："</span> <span class="token operator">+</span> t <span class="token operator">+</span> <span class="token string">"\nOffset："</span> <span class="token operator">+</span> o<span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="11"></td><td><pre>                            <span class="token punctuation">&#125;</span></pre></td></tr><tr><td data-num="12"></td><td><pre>                    <span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="13"></td><td><pre>                <span class="token punctuation">&#125;</span></pre></td></tr><tr><td data-num="14"></td><td><pre>            <span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr></table></figure><h2 id="五-kafka监控kafka-eagle"><a class="anchor" href="#五-kafka监控kafka-eagle">#</a> 五、Kafka 监控（Kafka Eagle）</h2><ol><li>修改 kafka 启动命令</li></ol><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre>修改kafka<span class="token operator">-</span>server<span class="token operator">-</span><span class="token keyword">start</span><span class="token punctuation">.</span>sh命令中</pre></td></tr><tr><td data-num="2"></td><td><pre></pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token comment">-- 原文：</span></pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token keyword">if</span> <span class="token punctuation">[</span> <span class="token string">"x$KAFKA_HEAP_OPTS"</span> <span class="token operator">=</span> <span class="token string">"x"</span> <span class="token punctuation">]</span><span class="token punctuation">;</span> <span class="token keyword">then</span></pre></td></tr><tr><td data-num="5"></td><td><pre>    export KAFKA_HEAP_OPTS<span class="token operator">=</span><span class="token string">"-Xmx1G -Xms1G"</span></pre></td></tr><tr><td data-num="6"></td><td><pre>fi</pre></td></tr><tr><td data-num="7"></td><td><pre></pre></td></tr><tr><td data-num="8"></td><td><pre><span class="token comment">-- 改为：</span></pre></td></tr><tr><td data-num="9"></td><td><pre><span class="token keyword">if</span> <span class="token punctuation">[</span> <span class="token string">"x$KAFKA_HEAP_OPTS"</span> <span class="token operator">=</span> <span class="token string">"x"</span> <span class="token punctuation">]</span><span class="token punctuation">;</span> <span class="token keyword">then</span></pre></td></tr><tr><td data-num="10"></td><td><pre>    export KAFKA_HEAP_OPTS<span class="token operator">=</span><span class="token string">"-server -Xms2G -Xmx2G -XX:PermSize=128m -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:ParallelGCThreads=8 -XX:ConcGCThreads=5 -XX:InitiatingHeapOccupancyPercent=70"</span></pre></td></tr><tr><td data-num="11"></td><td><pre>    export JMX_PORT<span class="token operator">=</span><span class="token string">"9999"</span></pre></td></tr><tr><td data-num="12"></td><td><pre>    <span class="token comment">#export KAFKA_HEAP_OPTS="-Xmx1G -Xms1G"</span></pre></td></tr><tr><td data-num="13"></td><td><pre>fi</pre></td></tr><tr><td data-num="14"></td><td><pre></pre></td></tr><tr><td data-num="15"></td><td><pre><span class="token comment">-- 注意：修改之后在启动 Kafka 之前要分发之其他节点</span></pre></td></tr></table></figure><ol start="2"><li>上传压缩包 kafka-eagle-bin-1.3.7.tar.gz 到集群 /opt/software 目录</li><li>解压到本地</li></ol><figure class="highlight java"><figcaption data-lang="java"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token punctuation">[</span>miyazono<span class="token annotation punctuation">@hadoop102</span> software<span class="token punctuation">]</span>$ tar <span class="token operator">-</span>zxvf kafka<span class="token operator">-</span>eagle<span class="token operator">-</span>bin<span class="token operator">-</span><span class="token number">1.3</span><span class="token number">.7</span><span class="token punctuation">.</span>tar<span class="token punctuation">.</span>gz</pre></td></tr></table></figure><ol start="4"><li>进入刚才解压的目录，将 kafka-eagle-web-1.3.7-bin.tar.gz 解压至 opt/module</li></ol><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token punctuation">[</span>miyazono<span class="token variable">@hadoop102</span> kafka<span class="token operator">-</span>eagle<span class="token operator">-</span>bin<span class="token operator">-</span><span class="token number">1.3</span><span class="token number">.7</span><span class="token punctuation">]</span> $ tar <span class="token operator">-</span>zxvf kafka<span class="token operator">-</span>eagle<span class="token operator">-</span>web<span class="token operator">-</span><span class="token number">1.4</span><span class="token number">.5</span><span class="token operator">-</span>bin<span class="token punctuation">.</span>tar<span class="token punctuation">.</span>gz <span class="token operator">-</span>C <span class="token operator">/</span>opt<span class="token operator">/</span>module<span class="token operator">/</span></pre></td></tr></table></figure><ol start="5"><li>修改名称</li></ol><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token punctuation">[</span>miyazono<span class="token variable">@hadoop102</span> module<span class="token punctuation">]</span>$ mv kafka<span class="token operator">-</span>eagle<span class="token operator">-</span>we<span class="token operator">-</span><span class="token number">1.4</span><span class="token number">.5</span><span class="token operator">/</span>   eagle</pre></td></tr></table></figure><ol start="6"><li>给启动文件执行权限 /opt/module/eagle/bin</li></ol><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token punctuation">[</span>miyazono<span class="token variable">@hadoop102</span> bin<span class="token punctuation">]</span>$ chmod <span class="token number">777</span> ke<span class="token punctuation">.</span>sh</pre></td></tr></table></figure><ol start="7"><li>修改配置文件 /opt/module/eagle/conf/system-config.properties</li></ol><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token comment">######################################</span></pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token comment"># multi zookeeper&amp;kafka cluster list</span></pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token comment">######################################</span></pre></td></tr><tr><td data-num="4"></td><td><pre>kafka<span class="token punctuation">.</span>eagle<span class="token punctuation">.</span>zk<span class="token punctuation">.</span>cluster<span class="token punctuation">.</span>alias<span class="token operator">=</span>cluster1</pre></td></tr><tr><td data-num="5"></td><td><pre>cluster1<span class="token punctuation">.</span>zk<span class="token punctuation">.</span>list<span class="token operator">=</span>hadoop102:<span class="token number">2181</span><span class="token punctuation">,</span>hadoop103:<span class="token number">2181</span><span class="token punctuation">,</span>hadoop104:<span class="token number">2181</span><span class="token operator">/</span>kafka</pre></td></tr><tr><td data-num="6"></td><td><pre></pre></td></tr><tr><td data-num="7"></td><td><pre></pre></td></tr><tr><td data-num="8"></td><td><pre><span class="token comment">######################################</span></pre></td></tr><tr><td data-num="9"></td><td><pre><span class="token comment"># kafka offset storage</span></pre></td></tr><tr><td data-num="10"></td><td><pre><span class="token comment">######################################</span></pre></td></tr><tr><td data-num="11"></td><td><pre>cluster1<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>eagle<span class="token punctuation">.</span><span class="token keyword">offset</span><span class="token punctuation">.</span>storage<span class="token operator">=</span>kafka</pre></td></tr><tr><td data-num="12"></td><td><pre></pre></td></tr><tr><td data-num="13"></td><td><pre></pre></td></tr><tr><td data-num="14"></td><td><pre><span class="token comment">######################################</span></pre></td></tr><tr><td data-num="15"></td><td><pre><span class="token comment"># kafka metrics, 30 days by default</span></pre></td></tr><tr><td data-num="16"></td><td><pre><span class="token comment">######################################</span></pre></td></tr><tr><td data-num="17"></td><td><pre>kafka<span class="token punctuation">.</span>eagle<span class="token punctuation">.</span>metrics<span class="token punctuation">.</span>charts<span class="token operator">=</span><span class="token boolean">true</span></pre></td></tr><tr><td data-num="18"></td><td><pre>kafka<span class="token punctuation">.</span>eagle<span class="token punctuation">.</span>metrics<span class="token punctuation">.</span>retain<span class="token operator">=</span><span class="token number">30</span></pre></td></tr><tr><td data-num="19"></td><td><pre></pre></td></tr><tr><td data-num="20"></td><td><pre></pre></td></tr><tr><td data-num="21"></td><td><pre><span class="token comment">######################################</span></pre></td></tr><tr><td data-num="22"></td><td><pre><span class="token comment"># kafka sqlite jdbc driver address</span></pre></td></tr><tr><td data-num="23"></td><td><pre></pre></td></tr><tr><td data-num="24"></td><td><pre><span class="token comment">######################################</span></pre></td></tr><tr><td data-num="25"></td><td><pre>kafka<span class="token punctuation">.</span>eagle<span class="token punctuation">.</span>driver<span class="token operator">=</span>com<span class="token punctuation">.</span>mysql<span class="token punctuation">.</span>jdbc<span class="token punctuation">.</span>Driver</pre></td></tr><tr><td data-num="26"></td><td><pre>kafka<span class="token punctuation">.</span>eagle<span class="token punctuation">.</span>url<span class="token operator">=</span>jdbc:mysql:<span class="token comment">//hadoop102:3306/ke?useUnicode=true&amp;characterEncoding=UTF-8&amp;zeroDateTimeBehavior=convertToNull</span></pre></td></tr><tr><td data-num="27"></td><td><pre>kafka<span class="token punctuation">.</span>eagle<span class="token punctuation">.</span>username<span class="token operator">=</span>root</pre></td></tr><tr><td data-num="28"></td><td><pre>kafka<span class="token punctuation">.</span>eagle<span class="token punctuation">.</span>password<span class="token operator">=</span><span class="token number">123456</span></pre></td></tr></table></figure><ol start="8"><li>添加环境变量</li></ol><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre>export KE_HOME<span class="token operator">=</span><span class="token operator">/</span>opt<span class="token operator">/</span>module<span class="token operator">/</span>eagle</pre></td></tr><tr><td data-num="2"></td><td><pre>export PATH<span class="token operator">=</span>$PATH:$KE_HOME<span class="token operator">/</span>bin</pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token comment">-- 注意：source /etc/profile</span></pre></td></tr></table></figure><ol start="9"><li>启动</li></ol><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token punctuation">[</span>miyazono<span class="token variable">@hadoop102</span> eagle<span class="token punctuation">]</span>$ bin<span class="token operator">/</span>ke<span class="token punctuation">.</span>sh <span class="token keyword">start</span></pre></td></tr><tr><td data-num="2"></td><td><pre></pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span></pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span></pre></td></tr><tr><td data-num="5"></td><td><pre><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span></pre></td></tr><tr><td data-num="6"></td><td><pre><span class="token operator">*</span> Kafka Eagle Service has started success<span class="token punctuation">.</span></pre></td></tr><tr><td data-num="7"></td><td><pre><span class="token operator">*</span> Welcome<span class="token punctuation">,</span> Now you can visit <span class="token string">'http://192.168.9.102:8048/ke'</span> <span class="token comment">-- 这个网址就是登入的 eagle 的网址</span></pre></td></tr><tr><td data-num="8"></td><td><pre><span class="token operator">*</span> Account:admin <span class="token punctuation">,</span>Password:<span class="token number">123456</span> <span class="token comment">-- 这是登入的密码</span></pre></td></tr><tr><td data-num="9"></td><td><pre><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span></pre></td></tr><tr><td data-num="10"></td><td><pre><span class="token operator">*</span> <span class="token operator">&lt;</span><span class="token keyword">Usage</span><span class="token operator">></span> ke<span class="token punctuation">.</span>sh <span class="token punctuation">[</span><span class="token keyword">start</span><span class="token operator">|</span><span class="token keyword">status</span><span class="token operator">|</span>stop<span class="token operator">|</span>restart<span class="token operator">|</span>stats<span class="token punctuation">]</span> <span class="token operator">&lt;</span><span class="token operator">/</span><span class="token keyword">Usage</span><span class="token operator">></span></pre></td></tr><tr><td data-num="11"></td><td><pre><span class="token operator">*</span> <span class="token operator">&lt;</span><span class="token keyword">Usage</span><span class="token operator">></span> https:<span class="token comment">//www.kafka-eagle.org/ &lt;/Usage></span></pre></td></tr><tr><td data-num="12"></td><td><pre><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span></pre></td></tr><tr><td data-num="13"></td><td><pre></pre></td></tr><tr><td data-num="14"></td><td><pre><span class="token comment">-- 注意：启动之前需要先启动 ZK 以及 KAFKA</span></pre></td></tr></table></figure><ol start="10"><li>登录页面查看监控数据</li></ol><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre>网址： http:<span class="token comment">//192.168.9.102:8048/ke</span></pre></td></tr><tr><td data-num="2"></td><td><pre>账号： admin</pre></td></tr><tr><td data-num="3"></td><td><pre>密码： <span class="token number">123456</span></pre></td></tr></table></figure><h2 id="六-面试题"><a class="anchor" href="#六-面试题">#</a> 六、面试题</h2><h3 id="61-kafka中的isr-ar代表什么"><a class="anchor" href="#61-kafka中的isr-ar代表什么">#</a> 6.1 Kafka 中的 ISR、AR 代表什么</h3><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre>ISR:分区leader维护的一个follower列表，在isr中的follower与leader同步。</pre></td></tr><tr><td data-num="2"></td><td><pre>AR:分区的所有副本。</pre></td></tr></table></figure><h3 id="62-kafka中的hw-leo等分别代表什么"><a class="anchor" href="#62-kafka中的hw-leo等分别代表什么">#</a> 6.2 Kafka 中的 HW、LEO 等分别代表什么</h3><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre>LEO: leader维护的isr中所有follower的最后偏移量。</pre></td></tr><tr><td data-num="2"></td><td><pre>HW：所有followerleo最小的值。</pre></td></tr></table></figure><h3 id="63-kafka中是怎么体现消息顺序性的"><a class="anchor" href="#63-kafka中是怎么体现消息顺序性的">#</a> 6.3 Kafka 中是怎么体现消息顺序性的</h3><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre>每次生产的数据是在一个上次生产数据的基础上追加，同时存储了消息的<span class="token keyword">offset</span>和数据的<span class="token keyword">index</span>索引，减少了数据存储时的磁头寻址的过程。</pre></td></tr></table></figure><h3 id="64-kafka中的分区器-序列化器-拦截器是否了解它们之间的处理顺序是什么"><a class="anchor" href="#64-kafka中的分区器-序列化器-拦截器是否了解它们之间的处理顺序是什么">#</a> 6.4 Kafka 中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？</h3><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre>处理顺序： 拦截器 <span class="token comment">--> 序列化器 --> 分区器</span></pre></td></tr><tr><td data-num="2"></td><td><pre>拦截器：对数据进行简单处理，加一些标识。</pre></td></tr><tr><td data-num="3"></td><td><pre>序列化：对数据进行序列化，保证数据可用于传输；</pre></td></tr><tr><td data-num="4"></td><td><pre>分区器：给数据加上分区标签，指定数据应该去到哪个kafka集群中的分区。</pre></td></tr><tr><td data-num="5"></td><td><pre></pre></td></tr><tr><td data-num="6"></td><td><pre>以上三步骤均在producer端就完成了。</pre></td></tr></table></figure><h3 id="65-kafka生产者客户端的整体结构是什么样子的使用了几个线程来处理分别是什么"><a class="anchor" href="#65-kafka生产者客户端的整体结构是什么样子的使用了几个线程来处理分别是什么">#</a> 6.5 Kafka 生产者客户端的整体结构是什么样子的？使用了几个线程来处理？分别是什么？</h3><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre>一共<span class="token number">2</span>个线程，一个数据缓存区。</pre></td></tr><tr><td data-num="2"></td><td><pre>线程</pre></td></tr><tr><td data-num="3"></td><td><pre>main线程：负责对数据进行包装、序列化、分区。</pre></td></tr><tr><td data-num="4"></td><td><pre>sender线程：负责将数据从数据缓冲区发送topic话题中。</pre></td></tr></table></figure><h3 id="66-消费者组中的消费者个数如果超过topic的分区那么就会有消费者消费不到数据这句话是否正确"><a class="anchor" href="#66-消费者组中的消费者个数如果超过topic的分区那么就会有消费者消费不到数据这句话是否正确">#</a> 6.6 消费者组中的消费者个数如果超过 topic 的分区，那么就会有消费者消费不到数据这句话是否正确</h3><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre>是的，正确。</pre></td></tr><tr><td data-num="2"></td><td><pre>了解一下分区分配的策略。</pre></td></tr><tr><td data-num="3"></td><td><pre>三种方式：roundrobin 、 range  、sticky。</pre></td></tr></table></figure><h3 id="67-消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset1"><a class="anchor" href="#67-消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset1">#</a> 6.7 消费者提交消费位移时提交的是当前消费到的最新消息的 offset 还是 offset+1？</h3><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">offset</span> <span class="token operator">+</span> <span class="token number">1</span> 。</pre></td></tr></table></figure><h3 id="68-有哪些情形会造成重复消费"><a class="anchor" href="#68-有哪些情形会造成重复消费">#</a> 6.8 有哪些情形会造成重复消费？</h3><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre>先消费后提交<span class="token keyword">offset</span>。</pre></td></tr></table></figure><h3 id="69-那些情景会造成消息漏消费"><a class="anchor" href="#69-那些情景会造成消息漏消费">#</a> 6.9 那些情景会造成消息漏消费？</h3><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre>先提交<span class="token keyword">offset</span>后消费。</pre></td></tr></table></figure><h3 id="610-当你使用kafka-topicssh创建删除了一个topic之后kafka背后会执行什么逻辑"><a class="anchor" href="#610-当你使用kafka-topicssh创建删除了一个topic之后kafka背后会执行什么逻辑">#</a> 6.10 当你使用 kafka-topics.sh 创建（删除）了一个 topic 之后，Kafka 背后会执行什么逻辑？</h3><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre>了解producer发送数据的过程。</pre></td></tr></table></figure><h3 id="611-topic的分区数可不可以增加如果可以怎么增加如果不可以那又是为什么"><a class="anchor" href="#611-topic的分区数可不可以增加如果可以怎么增加如果不可以那又是为什么">#</a> 6.11 topic 的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？</h3><pre><code>可以增加。
</code></pre><h3 id="612-topic的分区数可不可以减少如果可以怎么减少如果不可以那又是为什么"><a class="anchor" href="#612-topic的分区数可不可以减少如果可以怎么减少如果不可以那又是为什么">#</a> 6.12 topic 的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？</h3><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre>不能减少，因为原分区中的数据没有地方去。</pre></td></tr></table></figure><h3 id="613-kafka有内部的topic吗如果有是什么有什么作用"><a class="anchor" href="#613-kafka有内部的topic吗如果有是什么有什么作用">#</a> 6.13 Kafka 有内部的 topic 吗？如果有是什么？有什么作用？</h3><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre>会话：_consumer_offset，保存consumer消费的偏移量。</pre></td></tr></table></figure><h3 id="614-kafka分区分配的概念"><a class="anchor" href="#614-kafka分区分配的概念">#</a> 6.14 Kafka 分区分配的概念？</h3><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre>一共有三种分区分配的策略。</pre></td></tr><tr><td data-num="2"></td><td><pre>三种方式：</pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token number">1</span>）roundrobin ： 轮询分配。</pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token number">2</span>）range ： 平均分配。</pre></td></tr><tr><td data-num="5"></td><td><pre><span class="token number">3</span>）sticky ： 轮询分配 <span class="token operator">+</span> 解决新增消费者的优化。</pre></td></tr></table></figure><h3 id="615-简述kafka的日志目录结构"><a class="anchor" href="#615-简述kafka的日志目录结构">#</a> 6.15 简述 Kafka 的日志目录结构？</h3><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre>一共有<span class="token number">3</span>个文件</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token number">1</span>）log文件：记录真实数据，内部包含了真实数据 <span class="token operator">+</span> hw <span class="token operator">+</span> leo。</pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token number">2</span>）<span class="token keyword">index</span>文件 ： 存储消息的偏移量。</pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token number">3</span><span class="token punctuation">)</span> timeindex文件 ： 存储下消息的时间偏移量。</pre></td></tr></table></figure><h3 id="616-如果我指定了一个offsetkafka-controller怎么查找到对应的消息"><a class="anchor" href="#616-如果我指定了一个offsetkafka-controller怎么查找到对应的消息">#</a> 6.16 如果我指定了一个 offset，Kafka Controller 怎么查找到对应的消息？</h3><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre>通过<span class="token keyword">offset</span>，消息的偏移量，通过日志目录的文件顺序号，根据区间范围找到消息所在的inde和log目录。</pre></td></tr><tr><td data-num="2"></td><td><pre>其次根据在<span class="token keyword">index</span>表中的消息偏移量找到真实数据在log文件中该消息的起始索引位置。</pre></td></tr></table></figure><h3 id="617聊一聊kafka-controller的作用"><a class="anchor" href="#617聊一聊kafka-controller的作用">#</a> 6.17 聊一聊 Kafka Controller 的作用？</h3><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token number">1</span>、负责leader的选举；</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token number">2</span>、负责监控leader的状态；</pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token number">3.</span>负责更新集群在zookeeper中的状态。</pre></td></tr></table></figure><h3 id="618-kafka中有那些地方需要选举这些地方的选举策略又有哪些"><a class="anchor" href="#618-kafka中有那些地方需要选举这些地方的选举策略又有哪些">#</a> 6.18 Kafka 中有那些地方需要选举？这些地方的选举策略又有哪些？</h3><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token number">1.</span>每个分区的leader选举；<span class="token punctuation">(</span>isr<span class="token punctuation">)</span>；</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token number">2.</span>controller的选举（先到先得）。</pre></td></tr></table></figure><h3 id="619-失效副本是指什么有那些应对措施"><a class="anchor" href="#619-失效副本是指什么有那些应对措施">#</a> 6.19 失效副本是指什么？有那些应对措施？</h3><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre>follower不能与leader进行同步数据，暂时被leader踢出isr列表中。通过followe故障恢复重新备份，当leo达到了isr中的hw时，又重新会回到isr的列表中。</pre></td></tr></table></figure><h3 id="620-kafka的那些设计让它有如此高的性能"><a class="anchor" href="#620-kafka的那些设计让它有如此高的性能">#</a> 6.20 Kafka 的那些设计让它有如此高的性能？</h3><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token number">1.</span> pagecache；</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token number">2.</span>顺序读写机制；</pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token number">3.</span>零拷贝技术；</pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token number">4.</span>多分区策略。</pre></td></tr></table></figure><h2 id="七-flume与kafka融合技术"><a class="anchor" href="#七-flume与kafka融合技术">#</a> 七 、flume 与 kafka 融合技术</h2><p>kafka：数据的中转站，主要功能由 topic 体现；</p><p>flume：数据的采集，通过 source 和 sink 体现。</p><h3 id="71-kafka-source"><a class="anchor" href="#71-kafka-source">#</a> 7.1 kafka source</h3><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token comment">-- 问题 ：</span></pre></td></tr><tr><td data-num="2"></td><td><pre>fulme在kafka中的作用</pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token comment">-- 答案：</span></pre></td></tr><tr><td data-num="4"></td><td><pre>消费者</pre></td></tr></table></figure><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre>a1<span class="token punctuation">.</span>sources<span class="token punctuation">.</span>r1<span class="token punctuation">.</span><span class="token keyword">type</span> <span class="token operator">=</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flume<span class="token punctuation">.</span>source<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>KafkaSource <span class="token comment">--source 类型</span></pre></td></tr><tr><td data-num="2"></td><td><pre>a1<span class="token punctuation">.</span>sources<span class="token punctuation">.</span>r1<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>bootstrap<span class="token punctuation">.</span>servers <span class="token operator">=</span> hadoop105:<span class="token number">9092</span><span class="token punctuation">,</span>hadoop106:<span class="token number">9092</span> <span class="token comment">-- kafka 的集群</span></pre></td></tr><tr><td data-num="3"></td><td><pre>a1<span class="token punctuation">.</span>sources<span class="token punctuation">.</span>r1<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>topics<span class="token operator">=</span>topic_log <span class="token comment">-- 订阅的话题</span></pre></td></tr><tr><td data-num="4"></td><td><pre>a1<span class="token punctuation">.</span>sources<span class="token punctuation">.</span>r1<span class="token punctuation">.</span>batchSize<span class="token operator">=</span><span class="token number">6000</span> <span class="token comment">--putlist 中数据达到了 6K 以后提交到 channel 中</span></pre></td></tr><tr><td data-num="5"></td><td><pre>a1<span class="token punctuation">.</span>sources<span class="token punctuation">.</span>r1<span class="token punctuation">.</span>batchDurationMillis<span class="token operator">=</span><span class="token number">2000</span> <span class="token comment">-- 拉取数据的时间达到 2s 以后，将获取的数据提交到 channel 中</span></pre></td></tr></table></figure><h3 id="72-kakfa-channel"><a class="anchor" href="#72-kakfa-channel">#</a> 7.2 kakfa channel</h3><ul><li>kakfa channel 这种情况使用的最多，此时的 flume 可以是消费者、生产者、source 和 sink 之间的缓冲区（具有高吞吐量的优势），Channel 是位于 Source 和 Sink 之间的缓冲区。</li><li>一共有三种情况，分别是:</li></ul><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token comment">-- 情况一： 有 Flume source and sink -- 缓冲区</span></pre></td></tr><tr><td data-num="2"></td><td><pre>kakfa channel为事件提供了可靠且高可用的通道；</pre></td></tr><tr><td data-num="3"></td><td><pre></pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token comment">-- 情况二： 有 source and interceptor but no sink -- 生产者</span></pre></td></tr><tr><td data-num="5"></td><td><pre>it allows writing Flume events <span class="token keyword">into</span> a Kafka topic<span class="token punctuation">,</span> <span class="token keyword">for</span> <span class="token keyword">use</span> <span class="token keyword">by</span> other app</pre></td></tr><tr><td data-num="6"></td><td><pre></pre></td></tr><tr><td data-num="7"></td><td><pre><span class="token comment">-- 情况三： 有 sink, but no source -- 消费者</span></pre></td></tr><tr><td data-num="8"></td><td><pre>it <span class="token operator">is</span> a low<span class="token operator">-</span>latency<span class="token punctuation">,</span> fault tolerant way <span class="token keyword">to</span> send events <span class="token keyword">from</span> Kafka <span class="token keyword">to</span> Flume sinks such <span class="token keyword">as</span> HDFS<span class="token punctuation">,</span> HBase <span class="token operator">or</span> Solr</pre></td></tr></table></figure><p>官方配置文件：</p><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre>a1<span class="token punctuation">.</span>channels<span class="token punctuation">.</span>c1<span class="token punctuation">.</span><span class="token keyword">type</span> <span class="token operator">=</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flume<span class="token punctuation">.</span>channel<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>KafkaChannel <span class="token comment">----channel 类型</span></pre></td></tr><tr><td data-num="2"></td><td><pre>a1<span class="token punctuation">.</span>channels<span class="token punctuation">.</span>c1<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>bootstrap<span class="token punctuation">.</span>servers <span class="token operator">=</span> hadoop105:<span class="token number">9092</span><span class="token punctuation">,</span>hadoop106:<span class="token number">9092</span><span class="token punctuation">,</span>hadoop107:<span class="token number">9092</span> <span class="token comment">--kafka 集群</span></pre></td></tr><tr><td data-num="3"></td><td><pre>a1<span class="token punctuation">.</span>channels<span class="token punctuation">.</span>c1<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>topic <span class="token operator">=</span>topic_log <span class="token comment">-- 话题</span></pre></td></tr><tr><td data-num="4"></td><td><pre>a1<span class="token punctuation">.</span>channels<span class="token punctuation">.</span>c1<span class="token punctuation">.</span>parseAsFlumeEvent<span class="token operator">=</span><span class="token boolean">false</span> <span class="token comment">-- 不需要 event 的 header 数据</span></pre></td></tr></table></figure><h3 id="73-kafka-sink"><a class="anchor" href="#73-kafka-sink">#</a> 7.3 kafka sink</h3><p>作用：将数据拉去到 kafka 的 topic 中。</p><figure class="highlight sql"><figcaption data-lang="SQL"></figcaption><table><tr><td data-num="1"></td><td><pre>a1<span class="token punctuation">.</span>sinks<span class="token punctuation">.</span>k1<span class="token punctuation">.</span><span class="token keyword">type</span> <span class="token operator">=</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>flume<span class="token punctuation">.</span>sink<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>KafkaSink <span class="token comment">--sink 类型</span></pre></td></tr><tr><td data-num="2"></td><td><pre>a1<span class="token punctuation">.</span>sinks<span class="token punctuation">.</span>k1<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>topic <span class="token operator">=</span>topic_log <span class="token comment">-- 话题</span></pre></td></tr><tr><td data-num="3"></td><td><pre>a1<span class="token punctuation">.</span>sinks<span class="token punctuation">.</span>k1<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>bootstrap<span class="token punctuation">.</span>servers <span class="token operator">=</span> hadoop105:<span class="token number">9092</span><span class="token punctuation">,</span>hadoop106:<span class="token number">9092</span><span class="token punctuation">,</span>hadoop107:<span class="token number">9092</span> <span class="token comment">--kafka 集群</span></pre></td></tr><tr><td data-num="4"></td><td><pre>a1<span class="token punctuation">.</span>sinks<span class="token punctuation">.</span>k1<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>flumeBatchSize <span class="token operator">=</span> <span class="token number">20</span></pre></td></tr><tr><td data-num="5"></td><td><pre>a1<span class="token punctuation">.</span>sinks<span class="token punctuation">.</span>k1<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>producer<span class="token punctuation">.</span>acks <span class="token operator">=</span> <span class="token number">1</span> <span class="token comment">-- 副本策略</span></pre></td></tr><tr><td data-num="6"></td><td><pre>a1<span class="token punctuation">.</span>sinks<span class="token punctuation">.</span>k1<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>producer<span class="token punctuation">.</span>linger<span class="token punctuation">.</span>ms <span class="token operator">=</span> <span class="token number">1</span> </pre></td></tr><tr><td data-num="7"></td><td><pre>a1<span class="token punctuation">.</span>sinks<span class="token punctuation">.</span>k1<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>producer<span class="token punctuation">.</span>compression<span class="token punctuation">.</span><span class="token keyword">type</span> <span class="token operator">=</span> snappy  <span class="token comment">-- 压缩格式</span></pre></td></tr></table></figure></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i> </span><span class="text">更新于</span> <time title="修改时间：2021-09-29 23:01:39" itemprop="dateModified" datetime="2021-09-29T23:01:39+08:00">2021-09-29</time></span></div><div id="copyright"><ul><li class="author"><strong>本文作者： </strong>Miyazono <i class="ic i-at"><em>@</em></i>冬樱茶</li><li class="link"><strong>本文链接：</strong> <a href="https://github.com/Mayizono/miyazono.github.io/big-data/kafka/Kafka%E6%80%BB%E7%BB%93/" title="Kafka 学习">https://github.com/Mayizono/miyazono.github.io/big-data/kafka/Kafka总结/</a></li><li class="license"><strong>版权声明： </strong>本站所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> 许可协议。转载请注明出处！</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/big-data/zookeeper/zookeeper%E6%80%BB%E7%BB%93/" itemprop="url" rel="prev" data-background-image="https:&#x2F;&#x2F;tva4.sinaimg.cn&#x2F;mw690&#x2F;6833939bly1gipexe4oykj20zk0m87ji.jpg" title="zookeeper总结"><span class="type">上一篇</span> <span class="category"><i class="ic i-flag"></i> Zookeeper</span><h3>zookeeper总结</h3></a></div><div class="item right"><a href="/big-data/hadoop/hadoop/" itemprop="url" rel="next" data-background-image="https:&#x2F;&#x2F;tva4.sinaimg.cn&#x2F;mw690&#x2F;6833939bly1giph47e9vtj20zk0m8x6l.jpg" title="Hadoop学习"><span class="type">下一篇</span> <span class="category"><i class="ic i-flag"></i> Hadoop</span><h3>Hadoop学习</h3></a></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#kafka%E6%80%BB%E7%BB%93"><span class="toc-number">1.</span> <span class="toc-text">Kafka 总结</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80-kafka%E6%A6%82%E8%BF%B0"><span class="toc-number">1.1.</span> <span class="toc-text">一、kafka 概述</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#11-kafka%E5%AE%9A%E4%B9%89"><span class="toc-number">1.1.1.</span> <span class="toc-text">1.1 kafka 定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-%E6%A1%86%E6%9E%B6%E8%AF%B4%E6%98%8E"><span class="toc-number">1.1.2.</span> <span class="toc-text">1.2 框架说明</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-kafka%E6%B6%89%E5%8F%8A%E7%9A%84%E5%85%B3%E9%94%AE%E8%AF%8D"><span class="toc-number">1.1.3.</span> <span class="toc-text">1.3 Kafka 涉及的关键词</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C-kafka%E5%AE%89%E8%A3%85"><span class="toc-number">1.2.</span> <span class="toc-text">二、Kafka 安装</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#21-%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2"><span class="toc-number">1.2.1.</span> <span class="toc-text">2.1 集群部署</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#221-%E8%A7%A3%E5%8E%8B%E5%AE%89%E8%A3%85%E5%8C%85"><span class="toc-number">1.2.1.1.</span> <span class="toc-text">2.2.1 解压安装包</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#222-%E4%BF%AE%E6%94%B9%E8%A7%A3%E5%8E%8B%E5%90%8E%E7%9A%84%E6%96%87%E4%BB%B6%E5%90%8D%E7%A7%B0"><span class="toc-number">1.2.1.2.</span> <span class="toc-text">2.2.2 修改解压后的文件名称</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#223-%E5%88%9B%E5%BB%BAlogs%E6%96%87%E4%BB%B6%E5%A4%B9"><span class="toc-number">1.2.1.3.</span> <span class="toc-text">2.2.3 创建 logs 文件夹</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#224-%E4%BF%AE%E6%94%B9%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="toc-number">1.2.1.4.</span> <span class="toc-text">2.2.4 修改配置文件</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#225-%E9%85%8D%E7%BD%AE%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F"><span class="toc-number">1.2.1.5.</span> <span class="toc-text">2.2.5 配置环境变量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#226-%E5%88%86%E5%8F%91%E5%AE%89%E8%A3%85%E5%8C%85"><span class="toc-number">1.2.1.6.</span> <span class="toc-text">2.2.6 分发安装包</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#227-%E4%BF%AE%E6%94%B9%E5%85%B6%E4%BB%96%E6%9C%BA%E5%99%A8%E7%9A%84%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="toc-number">1.2.1.7.</span> <span class="toc-text">2.2.7 修改其他机器的配置文件</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#228-%E5%90%AF%E5%8A%A8%E9%9B%86%E7%BE%A4"><span class="toc-number">1.2.1.8.</span> <span class="toc-text">2.2.8 启动集群</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#229-%E5%85%B3%E9%97%AD%E9%9B%86%E7%BE%A4"><span class="toc-number">1.2.1.9.</span> <span class="toc-text">2.2.9 关闭集群</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2210-kafka%E7%BE%A4%E8%B5%B7%E7%BE%A4%E5%81%9C%E8%84%9A%E6%9C%AC"><span class="toc-number">1.2.1.10.</span> <span class="toc-text">2.2.10 kafka 群起群停脚本</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#22-kafka%E5%91%BD%E4%BB%A4%E6%93%8D%E4%BD%9C"><span class="toc-number">1.2.2.</span> <span class="toc-text">2.2 Kafka 命令操作</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#221-%E6%9F%A5%E7%9C%8B%E5%BD%93%E5%89%8D%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%AD%E7%9A%84%E6%89%80%E6%9C%89topic"><span class="toc-number">1.2.2.1.</span> <span class="toc-text">2.2.1 查看当前服务器中的所有 topic</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#222-%E5%88%9B%E5%BB%BAtopic"><span class="toc-number">1.2.2.2.</span> <span class="toc-text">2.2.2 创建 topic</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#223-%E6%9F%A5%E7%9C%8B%E6%9F%90%E4%B8%AAtopic%E7%9A%84%E8%AF%A6%E6%83%85"><span class="toc-number">1.2.2.3.</span> <span class="toc-text">2.2.3 查看某个 Topic 的详情</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#224-%E4%BF%AE%E6%94%B9%E5%88%86%E5%8C%BA%E6%95%B0"><span class="toc-number">1.2.2.4.</span> <span class="toc-text">2.2.4 修改分区数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#225-%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF"><span class="toc-number">1.2.2.5.</span> <span class="toc-text">2.2.5 发送消息</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#226-%E6%B6%88%E8%B4%B9%E6%B6%88%E6%81%AF"><span class="toc-number">1.2.2.6.</span> <span class="toc-text">2.2.6 消费消息</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#227-%E5%88%A0%E9%99%A4topic"><span class="toc-number">1.2.2.7.</span> <span class="toc-text">2.2.7 删除 topic</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89-kafka%E6%B7%B1%E5%85%A5%E6%B5%81%E7%A8%8B"><span class="toc-number">1.3.</span> <span class="toc-text">三、 Kafka 深入流程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#31-kafka%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B%E5%8F%8A%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8%E6%9C%BA%E5%88%B6"><span class="toc-number">1.3.1.</span> <span class="toc-text">3.1 Kafka 工作流程及文件存储机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#32-kafka%E4%B9%8B%E7%94%9F%E4%BA%A7%E8%80%85producer"><span class="toc-number">1.3.2.</span> <span class="toc-text">3.2 Kafka 之生产者 producer</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#321-%E5%88%86%E5%8C%BA%E7%AD%96%E7%95%A5"><span class="toc-number">1.3.2.1.</span> <span class="toc-text">3.2.1 分区策略</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#322-%E6%95%B0%E6%8D%AE%E5%8F%AF%E9%9D%A0%E6%80%A7%E4%BF%9D%E8%AF%81"><span class="toc-number">1.3.2.2.</span> <span class="toc-text">3.2.2 数据可靠性保证</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#33-kafka%E4%B9%8B%E6%B6%88%E8%B4%B9%E8%80%85-consumer"><span class="toc-number">1.3.3.</span> <span class="toc-text">3.3 Kafka 之消费者 consumer</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#331-%E6%B6%88%E8%B4%B9%E6%A8%A1%E5%BC%8F"><span class="toc-number">1.3.3.1.</span> <span class="toc-text">3.3.1 消费模式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#332-%E5%88%86%E5%8C%BA%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5"><span class="toc-number">1.3.3.2.</span> <span class="toc-text">3.3.2 分区分配策略</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#333-offset%E7%9A%84%E7%BB%B4%E6%8A%A4"><span class="toc-number">1.3.3.3.</span> <span class="toc-text">3.3.3 offset 的维护</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#34-kafka%E9%AB%98%E6%95%88%E8%AF%BB%E5%86%99%E6%95%B0%E6%8D%AE"><span class="toc-number">1.3.4.</span> <span class="toc-text">3.4 Kafka 高效读写数据</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#341-%E9%A1%BA%E5%BA%8F%E5%86%99%E7%A3%81%E7%9B%98"><span class="toc-number">1.3.4.1.</span> <span class="toc-text">3.4.1 顺序写磁盘</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#342-%E5%BA%94%E7%94%A8pagecache"><span class="toc-number">1.3.4.2.</span> <span class="toc-text">3.4.2 应用 Pagecache</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#343-%E9%9B%B6%E6%8B%B7%E8%B4%9D%E6%8A%80%E6%9C%AF"><span class="toc-number">1.3.4.3.</span> <span class="toc-text">3.4.3 零拷贝技术</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#35-zookeeper%E5%9C%A8kafka%E4%B8%AD%E7%9A%84%E4%BD%9C%E7%94%A8"><span class="toc-number">1.3.5.</span> <span class="toc-text">3.5 zookeeper 在 kafka 中的作用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#36-kafka%E4%BA%8B%E5%8A%A1"><span class="toc-number">1.3.6.</span> <span class="toc-text">3.6 Kafka 事务</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#361-producer%E4%BA%8B%E5%8A%A1"><span class="toc-number">1.3.6.1.</span> <span class="toc-text">3.6.1 producer 事务</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#362-consumer%E4%BA%8B%E5%8A%A1%E7%B2%BE%E5%87%86%E4%B8%80%E6%AC%A1%E6%80%A7%E6%B6%88%E8%B4%B9"><span class="toc-number">1.3.6.2.</span> <span class="toc-text">3.6.2 Consumer 事务（精准一次性消费）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B-kafka-api"><span class="toc-number">1.4.</span> <span class="toc-text">四、 Kafka API</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#41-producer-api"><span class="toc-number">1.4.1.</span> <span class="toc-text">4.1 Producer API</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#411-%E6%B6%88%E6%81%AF%E5%8F%91%E9%80%81%E6%B5%81%E7%A8%8B"><span class="toc-number">1.4.1.1.</span> <span class="toc-text">4.1.1 消息发送流程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#412-%E5%BC%82%E6%AD%A5%E5%8F%91%E9%80%81api"><span class="toc-number">1.4.1.2.</span> <span class="toc-text">4.1.2 异步发送 API</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#42-consumer-api"><span class="toc-number">1.4.2.</span> <span class="toc-text">4.2 Consumer API</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#421-%E6%95%B0%E6%8D%AE%E6%BC%8F%E6%B6%88%E8%B4%B9%E5%92%8C%E9%87%8D%E5%A4%8D%E6%B6%88%E8%B4%B9"><span class="toc-number">1.4.2.1.</span> <span class="toc-text">4.2.1 数据漏消费和重复消费</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#422-%E5%87%A0%E4%B8%AA%E9%87%8D%E8%A6%81%E7%9A%84%E5%8F%82%E6%95%B0"><span class="toc-number">1.4.2.2.</span> <span class="toc-text">4.2.2 几个重要的参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#423-%E4%BB%A3%E7%A0%81"><span class="toc-number">1.4.2.3.</span> <span class="toc-text">4.2.3 代码</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#consumer-api"><span class="toc-number">1.4.3.</span> <span class="toc-text">Consumer API</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94-kafka%E7%9B%91%E6%8E%A7kafka-eagle"><span class="toc-number">1.5.</span> <span class="toc-text">五、Kafka 监控（Kafka Eagle）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AD-%E9%9D%A2%E8%AF%95%E9%A2%98"><span class="toc-number">1.6.</span> <span class="toc-text">六、面试题</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#61-kafka%E4%B8%AD%E7%9A%84isr-ar%E4%BB%A3%E8%A1%A8%E4%BB%80%E4%B9%88"><span class="toc-number">1.6.1.</span> <span class="toc-text">6.1 Kafka 中的 ISR、AR 代表什么</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#62-kafka%E4%B8%AD%E7%9A%84hw-leo%E7%AD%89%E5%88%86%E5%88%AB%E4%BB%A3%E8%A1%A8%E4%BB%80%E4%B9%88"><span class="toc-number">1.6.2.</span> <span class="toc-text">6.2 Kafka 中的 HW、LEO 等分别代表什么</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#63-kafka%E4%B8%AD%E6%98%AF%E6%80%8E%E4%B9%88%E4%BD%93%E7%8E%B0%E6%B6%88%E6%81%AF%E9%A1%BA%E5%BA%8F%E6%80%A7%E7%9A%84"><span class="toc-number">1.6.3.</span> <span class="toc-text">6.3 Kafka 中是怎么体现消息顺序性的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#64-kafka%E4%B8%AD%E7%9A%84%E5%88%86%E5%8C%BA%E5%99%A8-%E5%BA%8F%E5%88%97%E5%8C%96%E5%99%A8-%E6%8B%A6%E6%88%AA%E5%99%A8%E6%98%AF%E5%90%A6%E4%BA%86%E8%A7%A3%E5%AE%83%E4%BB%AC%E4%B9%8B%E9%97%B4%E7%9A%84%E5%A4%84%E7%90%86%E9%A1%BA%E5%BA%8F%E6%98%AF%E4%BB%80%E4%B9%88"><span class="toc-number">1.6.4.</span> <span class="toc-text">6.4 Kafka 中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#65-kafka%E7%94%9F%E4%BA%A7%E8%80%85%E5%AE%A2%E6%88%B7%E7%AB%AF%E7%9A%84%E6%95%B4%E4%BD%93%E7%BB%93%E6%9E%84%E6%98%AF%E4%BB%80%E4%B9%88%E6%A0%B7%E5%AD%90%E7%9A%84%E4%BD%BF%E7%94%A8%E4%BA%86%E5%87%A0%E4%B8%AA%E7%BA%BF%E7%A8%8B%E6%9D%A5%E5%A4%84%E7%90%86%E5%88%86%E5%88%AB%E6%98%AF%E4%BB%80%E4%B9%88"><span class="toc-number">1.6.5.</span> <span class="toc-text">6.5 Kafka 生产者客户端的整体结构是什么样子的？使用了几个线程来处理？分别是什么？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#66-%E6%B6%88%E8%B4%B9%E8%80%85%E7%BB%84%E4%B8%AD%E7%9A%84%E6%B6%88%E8%B4%B9%E8%80%85%E4%B8%AA%E6%95%B0%E5%A6%82%E6%9E%9C%E8%B6%85%E8%BF%87topic%E7%9A%84%E5%88%86%E5%8C%BA%E9%82%A3%E4%B9%88%E5%B0%B1%E4%BC%9A%E6%9C%89%E6%B6%88%E8%B4%B9%E8%80%85%E6%B6%88%E8%B4%B9%E4%B8%8D%E5%88%B0%E6%95%B0%E6%8D%AE%E8%BF%99%E5%8F%A5%E8%AF%9D%E6%98%AF%E5%90%A6%E6%AD%A3%E7%A1%AE"><span class="toc-number">1.6.6.</span> <span class="toc-text">6.6 消费者组中的消费者个数如果超过 topic 的分区，那么就会有消费者消费不到数据这句话是否正确</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#67-%E6%B6%88%E8%B4%B9%E8%80%85%E6%8F%90%E4%BA%A4%E6%B6%88%E8%B4%B9%E4%BD%8D%E7%A7%BB%E6%97%B6%E6%8F%90%E4%BA%A4%E7%9A%84%E6%98%AF%E5%BD%93%E5%89%8D%E6%B6%88%E8%B4%B9%E5%88%B0%E7%9A%84%E6%9C%80%E6%96%B0%E6%B6%88%E6%81%AF%E7%9A%84offset%E8%BF%98%E6%98%AFoffset1"><span class="toc-number">1.6.7.</span> <span class="toc-text">6.7 消费者提交消费位移时提交的是当前消费到的最新消息的 offset 还是 offset+1？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#68-%E6%9C%89%E5%93%AA%E4%BA%9B%E6%83%85%E5%BD%A2%E4%BC%9A%E9%80%A0%E6%88%90%E9%87%8D%E5%A4%8D%E6%B6%88%E8%B4%B9"><span class="toc-number">1.6.8.</span> <span class="toc-text">6.8 有哪些情形会造成重复消费？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#69-%E9%82%A3%E4%BA%9B%E6%83%85%E6%99%AF%E4%BC%9A%E9%80%A0%E6%88%90%E6%B6%88%E6%81%AF%E6%BC%8F%E6%B6%88%E8%B4%B9"><span class="toc-number">1.6.9.</span> <span class="toc-text">6.9 那些情景会造成消息漏消费？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#610-%E5%BD%93%E4%BD%A0%E4%BD%BF%E7%94%A8kafka-topicssh%E5%88%9B%E5%BB%BA%E5%88%A0%E9%99%A4%E4%BA%86%E4%B8%80%E4%B8%AAtopic%E4%B9%8B%E5%90%8Ekafka%E8%83%8C%E5%90%8E%E4%BC%9A%E6%89%A7%E8%A1%8C%E4%BB%80%E4%B9%88%E9%80%BB%E8%BE%91"><span class="toc-number">1.6.10.</span> <span class="toc-text">6.10 当你使用 kafka-topics.sh 创建（删除）了一个 topic 之后，Kafka 背后会执行什么逻辑？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#611-topic%E7%9A%84%E5%88%86%E5%8C%BA%E6%95%B0%E5%8F%AF%E4%B8%8D%E5%8F%AF%E4%BB%A5%E5%A2%9E%E5%8A%A0%E5%A6%82%E6%9E%9C%E5%8F%AF%E4%BB%A5%E6%80%8E%E4%B9%88%E5%A2%9E%E5%8A%A0%E5%A6%82%E6%9E%9C%E4%B8%8D%E5%8F%AF%E4%BB%A5%E9%82%A3%E5%8F%88%E6%98%AF%E4%B8%BA%E4%BB%80%E4%B9%88"><span class="toc-number">1.6.11.</span> <span class="toc-text">6.11 topic 的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#612-topic%E7%9A%84%E5%88%86%E5%8C%BA%E6%95%B0%E5%8F%AF%E4%B8%8D%E5%8F%AF%E4%BB%A5%E5%87%8F%E5%B0%91%E5%A6%82%E6%9E%9C%E5%8F%AF%E4%BB%A5%E6%80%8E%E4%B9%88%E5%87%8F%E5%B0%91%E5%A6%82%E6%9E%9C%E4%B8%8D%E5%8F%AF%E4%BB%A5%E9%82%A3%E5%8F%88%E6%98%AF%E4%B8%BA%E4%BB%80%E4%B9%88"><span class="toc-number">1.6.12.</span> <span class="toc-text">6.12 topic 的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#613-kafka%E6%9C%89%E5%86%85%E9%83%A8%E7%9A%84topic%E5%90%97%E5%A6%82%E6%9E%9C%E6%9C%89%E6%98%AF%E4%BB%80%E4%B9%88%E6%9C%89%E4%BB%80%E4%B9%88%E4%BD%9C%E7%94%A8"><span class="toc-number">1.6.13.</span> <span class="toc-text">6.13 Kafka 有内部的 topic 吗？如果有是什么？有什么作用？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#614-kafka%E5%88%86%E5%8C%BA%E5%88%86%E9%85%8D%E7%9A%84%E6%A6%82%E5%BF%B5"><span class="toc-number">1.6.14.</span> <span class="toc-text">6.14 Kafka 分区分配的概念？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#615-%E7%AE%80%E8%BF%B0kafka%E7%9A%84%E6%97%A5%E5%BF%97%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84"><span class="toc-number">1.6.15.</span> <span class="toc-text">6.15 简述 Kafka 的日志目录结构？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#616-%E5%A6%82%E6%9E%9C%E6%88%91%E6%8C%87%E5%AE%9A%E4%BA%86%E4%B8%80%E4%B8%AAoffsetkafka-controller%E6%80%8E%E4%B9%88%E6%9F%A5%E6%89%BE%E5%88%B0%E5%AF%B9%E5%BA%94%E7%9A%84%E6%B6%88%E6%81%AF"><span class="toc-number">1.6.16.</span> <span class="toc-text">6.16 如果我指定了一个 offset，Kafka Controller 怎么查找到对应的消息？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#617%E8%81%8A%E4%B8%80%E8%81%8Akafka-controller%E7%9A%84%E4%BD%9C%E7%94%A8"><span class="toc-number">1.6.17.</span> <span class="toc-text">6.17 聊一聊 Kafka Controller 的作用？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#618-kafka%E4%B8%AD%E6%9C%89%E9%82%A3%E4%BA%9B%E5%9C%B0%E6%96%B9%E9%9C%80%E8%A6%81%E9%80%89%E4%B8%BE%E8%BF%99%E4%BA%9B%E5%9C%B0%E6%96%B9%E7%9A%84%E9%80%89%E4%B8%BE%E7%AD%96%E7%95%A5%E5%8F%88%E6%9C%89%E5%93%AA%E4%BA%9B"><span class="toc-number">1.6.18.</span> <span class="toc-text">6.18 Kafka 中有那些地方需要选举？这些地方的选举策略又有哪些？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#619-%E5%A4%B1%E6%95%88%E5%89%AF%E6%9C%AC%E6%98%AF%E6%8C%87%E4%BB%80%E4%B9%88%E6%9C%89%E9%82%A3%E4%BA%9B%E5%BA%94%E5%AF%B9%E6%8E%AA%E6%96%BD"><span class="toc-number">1.6.19.</span> <span class="toc-text">6.19 失效副本是指什么？有那些应对措施？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#620-kafka%E7%9A%84%E9%82%A3%E4%BA%9B%E8%AE%BE%E8%AE%A1%E8%AE%A9%E5%AE%83%E6%9C%89%E5%A6%82%E6%AD%A4%E9%AB%98%E7%9A%84%E6%80%A7%E8%83%BD"><span class="toc-number">1.6.20.</span> <span class="toc-text">6.20 Kafka 的那些设计让它有如此高的性能？</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%83-flume%E4%B8%8Ekafka%E8%9E%8D%E5%90%88%E6%8A%80%E6%9C%AF"><span class="toc-number">1.7.</span> <span class="toc-text">七 、flume 与 kafka 融合技术</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#71-kafka-source"><span class="toc-number">1.7.1.</span> <span class="toc-text">7.1 kafka source</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#72-kakfa-channel"><span class="toc-number">1.7.2.</span> <span class="toc-text">7.2 kakfa channel</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#73-kafka-sink"><span class="toc-number">1.7.3.</span> <span class="toc-text">7.3 kafka sink</span></a></li></ol></li></ol></li></ol></div><div class="related panel pjax" data-title="系列文章"><ul><li class="active"><a href="/big-data/kafka/Kafka%E6%80%BB%E7%BB%93/" rel="bookmark" title="Kafka学习">Kafka学习</a></li><li><a href="/big-data/kafka/Kafka%E3%80%81Redis%E3%80%81Pulsar%E5%AF%B9%E6%AF%94/" rel="bookmark" title="Kafka、Redis、Pulsar对比">Kafka、Redis、Pulsar对比</a></li></ul></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="Miyazono" data-src="/images/avatar.jpg"><p class="name" itemprop="name">Miyazono</p><div class="description" itemprop="description"></div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">22</span> <span class="name">文章</span></a></div><div class="item categories"><a href="/categories/"><span class="count">9</span> <span class="name">分类</span></a></div></nav><div class="social"><a href="https://github.com/Mayizono" title="https:&#x2F;&#x2F;github.com&#x2F;Mayizono" class="item github"><i class="ic i-github"></i></a> <span class="exturl item music" data-url="aHR0cHM6Ly9tdXNpYy4xNjMuY29tLyMvdXNlci9ob21lP2lkPTEzNDMzNjQzOQ==" title="https:&#x2F;&#x2F;music.163.com&#x2F;#&#x2F;user&#x2F;home?id&#x3D;134336439"><i class="ic i-cloud-music"></i></span> <span class="exturl item weibo" data-url="aHR0cHM6Ly93ZWliby5jb20vdS8yOTAwNzY4Nzky" title="https:&#x2F;&#x2F;weibo.com&#x2F;u&#x2F;2900768792"><i class="ic i-weibo"></i></span> <span class="exturl item email" data-url="aHR0cHM6Ly94eW5zdHVkeUAxNjMuY29t" title="https:&#x2F;&#x2F;xynstudy@163.com"><i class="ic i-envelope"></i></span></div><ul class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-feather"></i>文章</a><ul class="submenu"><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a></li><li class="item"><a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a></li><li class="item"><a href="/tags/" rel="section"><i class="ic i-tags"></i>标签</a></li></ul></li><li class="item"><a href="/friends/" rel="section"><i class="ic i-heart"></i>friends</a></li><li class="item"><a href="/mikutap/" rel="section"><i class="ic i-star"></i>mikutap</a></li></ul></div></div></div><ul id="quick"><li class="prev pjax"><a href="/big-data/zookeeper/zookeeper%E6%80%BB%E7%BB%93/" rel="prev" title="上一篇"><i class="ic i-chevron-left"></i></a></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/big-data/hadoop/hadoop/" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>随机文章</h2><ul><li class="item"><div class="breadcrumb"><a href="/categories/big-data/" title="分类于 大数据">大数据</a> <i class="ic i-angle-right"></i> <a href="/categories/big-data/kafka/" title="分类于 Kafka">Kafka</a></div><span><a href="/big-data/kafka/Kafka%E6%80%BB%E7%BB%93/" title="Kafka学习">Kafka学习</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/big-data/" title="分类于 大数据">大数据</a> <i class="ic i-angle-right"></i> <a href="/categories/big-data/hive/" title="分类于 Hive">Hive</a></div><span><a href="/big-data/hive/Hive%20%E5%AD%A6%E4%B9%A0/" title="Hive学习">Hive学习</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/big-data/" title="分类于 大数据">大数据</a> <i class="ic i-angle-right"></i> <a href="/categories/big-data/hadoop/" title="分类于 Hadoop">Hadoop</a></div><span><a href="/big-data/hadoop/hadoop/" title="Hadoop学习">Hadoop学习</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/big-data/" title="分类于 大数据">大数据</a> <i class="ic i-angle-right"></i> <a href="/categories/big-data/flink/" title="分类于 Flink">Flink</a></div><span><a href="/big-data/flink/Flink%E7%9A%84%E6%9E%B6%E6%9E%84/" title="Flink的架构">Flink的架构</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/big-data/" title="分类于 大数据">大数据</a> <i class="ic i-angle-right"></i> <a href="/categories/big-data/pro/" title="分类于 大厂学习">大厂学习</a></div><span><a href="/big-data/pro/%E6%BB%B4%E6%BB%B4%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97%E5%8F%91%E5%B1%95%E4%B9%8B%E8%B7%AF%E5%8F%8A%E5%B9%B3%E5%8F%B0%E6%9E%B6%E6%9E%84%E5%AE%9E%E8%B7%B5/" title="滴滴实时计算发展之路及平台架构实践">滴滴实时计算发展之路及平台架构实践</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/big-data/" title="分类于 大数据">大数据</a> <i class="ic i-angle-right"></i> <a href="/categories/big-data/flume/" title="分类于 Flume">Flume</a></div><span><a href="/big-data/flume/Flume%E7%BB%83%E4%B9%A0/" title="Flume练习题">Flume练习题</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/big-data/" title="分类于 大数据">大数据</a> <i class="ic i-angle-right"></i> <a href="/categories/big-data/flink/" title="分类于 Flink">Flink</a></div><span><a href="/big-data/flink/Flink%E5%85%B3%E8%81%94%E7%BB%B4%E8%A1%A8%E6%96%B9%E6%A1%88/" title="Flink关联维表方案">Flink关联维表方案</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/big-data/" title="分类于 大数据">大数据</a> <i class="ic i-angle-right"></i> <a href="/categories/big-data/flink/" title="分类于 Flink">Flink</a></div><span><a href="/big-data/flink/Flink%E7%AE%A1%E7%90%86Kafka%E7%9A%84%E6%B6%88%E8%B4%B9%E4%BD%8D%E7%82%B9/" title="Flink管理 Kafka 消费位点">Flink管理 Kafka 消费位点</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/big-data/" title="分类于 大数据">大数据</a> <i class="ic i-angle-right"></i> <a href="/categories/big-data/zookeeper/" title="分类于 Zookeeper">Zookeeper</a></div><span><a href="/big-data/zookeeper/zookeeper%E6%80%BB%E7%BB%93/" title="zookeeper总结">zookeeper总结</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/big-data/" title="分类于 大数据">大数据</a> <i class="ic i-angle-right"></i> <a href="/categories/big-data/hive/" title="分类于 Hive">Hive</a></div><span><a href="/big-data/hive/leetcode%E6%95%B0%E6%8D%AE%E5%BA%93%E9%A2%98%E7%9B%AE1-123%E9%A2%98%EF%BC%8820-08-14)/" title="力扣数据库">力扣数据库</a></span></li></ul></div><div><h2>最新评论</h2><ul class="leancloud-recent-comment"></ul></div></div><div class="status"><div class="copyright">&copy; 2018 – <span itemprop="copyrightYear">2021</span> <span class="with-love"><i class="ic i-sakura rotate"></i> </span><span class="author" itemprop="copyrightHolder">Miyazono @ Yume Shoka</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i> </span><span title="站点总字数">310k 字</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="ic i-coffee"></i> </span><span title="站点阅读时长">4:42</span></div><div class="powered-by">基于 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<a href="https://github.com/amehime/hexo-theme-shoka">Shoka</a></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"big-data/kafka/Kafka总结/",favicon:{show:"（●´3｀●）やれやれだぜ",hide:"(´Д｀)大変だ！"},search:{placeholder:"文章搜索",empty:"关于 「 ${query} 」，什么也没搜到",stats:"${time} ms 内找到 ${hits} 条结果"},valine:!0,fancybox:!0,copyright:'复制成功，转载请遵守 <i class="ic i-creative-commons"></i>BY-NC-SA 协议。',ignores:[function(a){return a.includes("#")},function(a){return new RegExp(LOCAL.path+"$").test(a)}]}</script><script src="https://cdn.polyfill.io/v2/polyfill.js"></script><script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script><script src="/js/app.js?v=0.2.5"></script></body></html><!-- rebuild by hrmmi -->